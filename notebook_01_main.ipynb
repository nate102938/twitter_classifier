{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e9541f4",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The goal of this project is to perform \n",
    "\n",
    "The goal of this project is to perform a sentiment analysis of Apple customers, and uncover actionable insight that could be used to optimize a marketing strategy going forward. To achieve this, we built a predictive model using Natural Language Processing (NLP), that could rate the sentiment of a tweet based on its content. At the end of our analysis, we present the findings of our model and provide concrete recommendations as to how Apple could improve its marketing strategy going forward and ultimately increase customer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6f351",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "Developing an excellent marketing strategy is crucial \n",
    "\n",
    "\n",
    "Developing an excellent marketing strategy is crucial for an organization to consistently achieve positive results. To perform effective marketing, companies need to gain a deep understanding of their customers and uncover what matters to them most. The challenge is figuring out how to gain this insight in an efficient manner, and how to consistently implement meaningful change. Fortunately, machine learning provides us with unique and effective tools to perform customer sentiment analysis and guide long-term decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f6bda1",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "\n",
    "For this analysis, I utilized tweet data from 115,511 tweets from 587 Twitter accounts that were pulled from the Twitter API.  These accounts were manually selected by me to represent each account class that I am trying to predict.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84883c8",
   "metadata": {},
   "source": [
    "## Imports / settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e93d8154",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:23:02.352795Z",
     "start_time": "2023-02-15T20:23:00.444101Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\natek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\natek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\natek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import string\n",
    "\n",
    "# Twitter import\n",
    "import tweepy\n",
    "\n",
    "# Analysis imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP imports\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "# SKlearn imports\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_colwidth = 90\n",
    "\n",
    "# Downloads (for NLP)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349fbe4",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78df34d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:23:04.767460Z",
     "start_time": "2023-02-15T20:23:04.756016Z"
    }
   },
   "outputs": [],
   "source": [
    "tweet_list_file = 'tweet_list2.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e3762",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "These are helper functions that assist in the manipulation of tweet strings for pre-processing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8694f2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:23:05.767243Z",
     "start_time": "2023-02-15T20:23:05.738012Z"
    }
   },
   "outputs": [],
   "source": [
    "def strip_rt_user(text):\n",
    "    if text[0:2] == \"RT\":\n",
    "        colon = text.find(\":\")\n",
    "        return text[colon+1:].lower()\n",
    "    else:\n",
    "        return text.lower()\n",
    "\n",
    "def get_rt_user(text):\n",
    "    if text[0:2] == \"RT\":\n",
    "        colon = text.find(\":\")\n",
    "        user = text[:colon]\n",
    "        at = user.find(\"@\")\n",
    "        return (user[at+1:]).lower()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def addHashTags(text):\n",
    "    return \"#\" + text + \"#\"\n",
    "\n",
    "# Translate nltk POS to wordnet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def remove_characters(text, char_to_remove):\n",
    "    str1 = ''.join(x for x in text if not x in char_to_remove)\n",
    "    return str1\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = remove_characters(text, string.punctuation)\n",
    "    return text\n",
    "\n",
    "def tag_and_lemmatize(text):\n",
    "    newText = text\n",
    "    newText = pos_tag(newText)\n",
    "    newText = [(x[0], get_wordnet_pos(x[1])) for x in newText]\n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "    newText = [(lemma.lemmatize(x[0], x[1])) for x in newText]\n",
    "    return newText\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "# perform all pre-processing on a df\n",
    "def preprocessing(df):\n",
    "    preprocessing_01_model_specific(df)\n",
    "    preprocessing_02_general(df)\n",
    "    preprocessing_03_tag_and_lemmatize(df)\n",
    "    \n",
    "    \n",
    "def preprocessing_01_model_specific(df):\n",
    "    # Copy the RT user name from the text column and put it into a different column.\n",
    "    df['RT_user'] = df['text'].apply(get_rt_user)\n",
    "    df['RT_user'] = df['RT_user'].apply(lambda x: addHashTags(x) if x != \"\" else \"\")\n",
    "\n",
    "    # Pull out the RT user name from the text column\n",
    "    df['text'] = df['text'].apply(strip_rt_user)\n",
    "    \n",
    "def preprocessing_02_general(df):\n",
    "    # Lower case the text tweets\n",
    "    df['text'] = df['text'].str.lower()\n",
    "\n",
    "    # Strip out the meaningless links\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join([n for n in x.split() if n[0:4] != \"http\"]))\n",
    "\n",
    "    # Strip any excess white space\n",
    "    df['text'] = df['text'].apply(lambda x: x.strip())\n",
    "    \n",
    "    # Take out stop words\n",
    "    sw = set(stopwords.words('english'))\n",
    "    sw.update(['amp'])\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join([n for n in x.split() if n not in sw]))\n",
    "\n",
    "    # Remove punctuation\n",
    "    df['text'] = df['text'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "    # Make sure we don't have any random numbers\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join([n for n in x.split() if n.isnumeric() == False]))\n",
    "\n",
    "    # Put together the RT user and the tweet text\n",
    "    df['text'] = df['text'] + \" \" + df['RT_user']\n",
    "\n",
    "    # Make a new column, tokenize the words\n",
    "    df['text_tokenized'] = df['text'].str.split()\n",
    "    \n",
    "    df = df.drop(columns=['id', 'author_id', 'created_at'])\n",
    "    \n",
    "    df['text'] = df['text'].apply(lambda x: np.nan if len(x.strip()) == 0 else x)\n",
    "    df = df.dropna().reset_index(drop=True) \n",
    "\n",
    "    le = LabelEncoder()\n",
    "    df['class_label'] = le.fit_transform(df['class'])\n",
    "    df.head()\n",
    "    \n",
    "def preprocessing_03_tag_and_lemmatize(df):\n",
    "    df['text_tokenized'] = df['text_tokenized'].apply(tag_and_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5523bcf2",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b2713",
   "metadata": {},
   "source": [
    "Data collection methods and code is located in a separate notebook linked ([here](notebook_02_data_collection.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f818b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:33:35.951004Z",
     "start_time": "2023-02-15T05:33:35.933045Z"
    }
   },
   "source": [
    "## Load tweet data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08b6f7",
   "metadata": {},
   "source": [
    "Load the tweet data from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a0d82a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:23:07.891119Z",
     "start_time": "2023-02-15T20:23:07.438335Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620527449326108672</td>\n",
       "      <td>On this day 83 years ago, Democrats Delivered the first Social Security checks ever!  ...</td>\n",
       "      <td>2461810448</td>\n",
       "      <td>2023-01-31 21:00:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620131183597359104</td>\n",
       "      <td>We must keep our children safe from gun violence. Safe storage of guns saves lives and...</td>\n",
       "      <td>2461810448</td>\n",
       "      <td>2023-01-30 18:45:49+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1619445261784477696</td>\n",
       "      <td>Democrats believe that health care is a human right  and #DemocratsDelivered help for ...</td>\n",
       "      <td>2461810448</td>\n",
       "      <td>2023-01-28 21:20:12+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1619183614050156544</td>\n",
       "      <td>Congratulations @PADems for your hard-won victories electing Pennsylvania Democrats wh...</td>\n",
       "      <td>2461810448</td>\n",
       "      <td>2023-01-28 04:00:31+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1619157193626116098</td>\n",
       "      <td>My heart goes out to Tyre Nichols mother and their entire family. Tyre should be alive...</td>\n",
       "      <td>2461810448</td>\n",
       "      <td>2023-01-28 02:15:32+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_name               class                   id  \\\n",
       "0  TeamPelosi  Politics - Liberal  1620527449326108672   \n",
       "1  TeamPelosi  Politics - Liberal  1620131183597359104   \n",
       "2  TeamPelosi  Politics - Liberal  1619445261784477696   \n",
       "3  TeamPelosi  Politics - Liberal  1619183614050156544   \n",
       "4  TeamPelosi  Politics - Liberal  1619157193626116098   \n",
       "\n",
       "                                                                                        text  \\\n",
       "0  On this day 83 years ago, Democrats Delivered the first Social Security checks ever!  ...   \n",
       "1  We must keep our children safe from gun violence. Safe storage of guns saves lives and...   \n",
       "2  Democrats believe that health care is a human right  and #DemocratsDelivered help for ...   \n",
       "3  Congratulations @PADems for your hard-won victories electing Pennsylvania Democrats wh...   \n",
       "4  My heart goes out to Tyre Nichols mother and their entire family. Tyre should be alive...   \n",
       "\n",
       "    author_id                 created_at  \n",
       "0  2461810448  2023-01-31 21:00:26+00:00  \n",
       "1  2461810448  2023-01-30 18:45:49+00:00  \n",
       "2  2461810448  2023-01-28 21:20:12+00:00  \n",
       "3  2461810448  2023-01-28 04:00:31+00:00  \n",
       "4  2461810448  2023-01-28 02:15:32+00:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tweets from file\n",
    "df = pd.read_csv(tweet_list_file)\n",
    "\n",
    "# Format all series as strings\n",
    "for n in df.columns:\n",
    "    df[n] = df[n].astype(str)\n",
    "\n",
    "# Check out the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b25782",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf01a3c2",
   "metadata": {},
   "source": [
    "**Check for nulls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e69a20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:23:09.208718Z",
     "start_time": "2023-02-15T20:23:09.155856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84908 entries, 0 to 84907\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   user_name   84908 non-null  object\n",
      " 1   class       84908 non-null  object\n",
      " 2   id          84908 non-null  object\n",
      " 3   text        84908 non-null  object\n",
      " 4   author_id   84908 non-null  object\n",
      " 5   created_at  84908 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d99627",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- There are no null values, which makes sense because I downloaded this data myself. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9b3386",
   "metadata": {},
   "source": [
    "**Check for duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e566648",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:23:09.852877Z",
     "start_time": "2023-02-15T20:23:09.730245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "803"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7118a217",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- I have some duplicate tweets.  As I noted in the data collection notebook, I must have downloaded some tweets from the same account multiple times while performing the download function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbe5b82",
   "metadata": {},
   "source": [
    "**Drop duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4725b7bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:23:13.443466Z",
     "start_time": "2023-02-15T20:23:13.224860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c3ac3",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Duplicates have been deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b12635f",
   "metadata": {},
   "source": [
    "## Data review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5040948c",
   "metadata": {},
   "source": [
    "Check class balance at the tweet level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af36859f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:23:15.209427Z",
     "start_time": "2023-02-15T20:23:15.185290Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics - Conservative    15500\n",
       "TV / movies                12007\n",
       "Politics - Liberal         12001\n",
       "Sports                     12000\n",
       "Music                      11600\n",
       "Business and finance        8452\n",
       "Science / Technology        7550\n",
       "Travel                      4995\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddaf796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T06:29:25.411417Z",
     "start_time": "2023-02-15T06:29:25.399488Z"
    }
   },
   "source": [
    "Notes: \n",
    "- It's imbalanced but I'm going to leave it and see if we can still make predictions from the data we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05711522",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ded33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T19:56:38.197672Z",
     "start_time": "2023-02-15T19:56:38.191658Z"
    }
   },
   "source": [
    "## Model 1 - Predict primary interest of account from 6 major categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edde8d9",
   "metadata": {},
   "source": [
    "### Pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c46cf",
   "metadata": {},
   "source": [
    "**Warning** This code performs all pre-processing, including lemmatization of the tweet text.  As such, it takes a few minutes to run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43e090fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T19:44:43.160116Z",
     "start_time": "2023-02-15T19:43:06.855669Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>RT_user</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620527449326108672</td>\n",
       "      <td>day years ago democrats delivered first social security checks ever republicans social...</td>\n",
       "      <td>2461810448</td>\n",
       "      <td>2023-01-31 21:00:26+00:00</td>\n",
       "      <td></td>\n",
       "      <td>[day, year, ago, democrat, deliver, first, social, security, check, ever, republicans,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620131183597359104</td>\n",
       "      <td>must keep children safe gun violence safe storage guns saves lives heartbreak thats ho...</td>\n",
       "      <td>2461810448</td>\n",
       "      <td>2023-01-30 18:45:49+00:00</td>\n",
       "      <td></td>\n",
       "      <td>[must, keep, child, safe, gun, violence, safe, storage, gun, save, life, heartbreak, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1619445261784477696</td>\n",
       "      <td>democrats believe health care human right democratsdelivered help families get coverag...</td>\n",
       "      <td>2461810448</td>\n",
       "      <td>2023-01-28 21:20:12+00:00</td>\n",
       "      <td></td>\n",
       "      <td>[democrat, believe, health, care, human, right, democratsdelivered, help, family, get,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1619183614050156544</td>\n",
       "      <td>congratulations padems hardwon victories electing pennsylvania democrats committed gro...</td>\n",
       "      <td>2461810448</td>\n",
       "      <td>2023-01-28 04:00:31+00:00</td>\n",
       "      <td></td>\n",
       "      <td>[congratulation, padems, hardwon, victory, elect, pennsylvania, democrat, commit, grow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1619157193626116098</td>\n",
       "      <td>heart goes tyre nichols mother entire family tyre alive today justice must done must r...</td>\n",
       "      <td>2461810448</td>\n",
       "      <td>2023-01-28 02:15:32+00:00</td>\n",
       "      <td></td>\n",
       "      <td>[heart, go, tyre, nichols, mother, entire, family, tyre, alive, today, justice, must, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84903</th>\n",
       "      <td>BBC_Travel</td>\n",
       "      <td>Travel</td>\n",
       "      <td>1317805131052634115</td>\n",
       "      <td>pictureperfect medieval city may show us live better</td>\n",
       "      <td>173992307</td>\n",
       "      <td>2020-10-18 12:30:01+00:00</td>\n",
       "      <td></td>\n",
       "      <td>[pictureperfect, medieval, city, may, show, u, live, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84904</th>\n",
       "      <td>BBC_Travel</td>\n",
       "      <td>Travel</td>\n",
       "      <td>1317586183845605377</td>\n",
       "      <td>stunning alpine landscape lead nietzsche proclaim god dead</td>\n",
       "      <td>173992307</td>\n",
       "      <td>2020-10-17 22:00:00+00:00</td>\n",
       "      <td></td>\n",
       "      <td>[stun, alpine, landscape, lead, nietzsche, proclaim, god, dead]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84905</th>\n",
       "      <td>BBC_Travel</td>\n",
       "      <td>Travel</td>\n",
       "      <td>1317442744374079491</td>\n",
       "      <td>writing system world used exclusively women</td>\n",
       "      <td>173992307</td>\n",
       "      <td>2020-10-17 12:30:01+00:00</td>\n",
       "      <td></td>\n",
       "      <td>[write, system, world, use, exclusively, woman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84906</th>\n",
       "      <td>BBC_Travel</td>\n",
       "      <td>Travel</td>\n",
       "      <td>1317223800035971072</td>\n",
       "      <td>steak spaghetti ketchup postwar japan yoshoku become countrys defacto comfort food act...</td>\n",
       "      <td>173992307</td>\n",
       "      <td>2020-10-16 22:00:01+00:00</td>\n",
       "      <td></td>\n",
       "      <td>[steak, spaghetti, ketchup, postwar, japan, yoshoku, become, countrys, defacto, comfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84907</th>\n",
       "      <td>BBC_Travel</td>\n",
       "      <td>Travel</td>\n",
       "      <td>1317163398224699392</td>\n",
       "      <td>one countrys leastvisited prefectures chefs dramatically sear bonito towering flames</td>\n",
       "      <td>173992307</td>\n",
       "      <td>2020-10-16 18:00:00+00:00</td>\n",
       "      <td></td>\n",
       "      <td>[one, country, leastvisited, prefecture, chef, dramatically, sear, bonito, towering, f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84105 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_name               class                   id  \\\n",
       "0      TeamPelosi  Politics - Liberal  1620527449326108672   \n",
       "1      TeamPelosi  Politics - Liberal  1620131183597359104   \n",
       "2      TeamPelosi  Politics - Liberal  1619445261784477696   \n",
       "3      TeamPelosi  Politics - Liberal  1619183614050156544   \n",
       "4      TeamPelosi  Politics - Liberal  1619157193626116098   \n",
       "...           ...                 ...                  ...   \n",
       "84903  BBC_Travel              Travel  1317805131052634115   \n",
       "84904  BBC_Travel              Travel  1317586183845605377   \n",
       "84905  BBC_Travel              Travel  1317442744374079491   \n",
       "84906  BBC_Travel              Travel  1317223800035971072   \n",
       "84907  BBC_Travel              Travel  1317163398224699392   \n",
       "\n",
       "                                                                                            text  \\\n",
       "0      day years ago democrats delivered first social security checks ever republicans social...   \n",
       "1      must keep children safe gun violence safe storage guns saves lives heartbreak thats ho...   \n",
       "2      democrats believe health care human right democratsdelivered help families get coverag...   \n",
       "3      congratulations padems hardwon victories electing pennsylvania democrats committed gro...   \n",
       "4      heart goes tyre nichols mother entire family tyre alive today justice must done must r...   \n",
       "...                                                                                          ...   \n",
       "84903                                      pictureperfect medieval city may show us live better    \n",
       "84904                                stunning alpine landscape lead nietzsche proclaim god dead    \n",
       "84905                                               writing system world used exclusively women    \n",
       "84906  steak spaghetti ketchup postwar japan yoshoku become countrys defacto comfort food act...   \n",
       "84907      one countrys leastvisited prefectures chefs dramatically sear bonito towering flames    \n",
       "\n",
       "        author_id                 created_at RT_user  \\\n",
       "0      2461810448  2023-01-31 21:00:26+00:00           \n",
       "1      2461810448  2023-01-30 18:45:49+00:00           \n",
       "2      2461810448  2023-01-28 21:20:12+00:00           \n",
       "3      2461810448  2023-01-28 04:00:31+00:00           \n",
       "4      2461810448  2023-01-28 02:15:32+00:00           \n",
       "...           ...                        ...     ...   \n",
       "84903   173992307  2020-10-18 12:30:01+00:00           \n",
       "84904   173992307  2020-10-17 22:00:00+00:00           \n",
       "84905   173992307  2020-10-17 12:30:01+00:00           \n",
       "84906   173992307  2020-10-16 22:00:01+00:00           \n",
       "84907   173992307  2020-10-16 18:00:00+00:00           \n",
       "\n",
       "                                                                                  text_tokenized  \n",
       "0      [day, year, ago, democrat, deliver, first, social, security, check, ever, republicans,...  \n",
       "1      [must, keep, child, safe, gun, violence, safe, storage, gun, save, life, heartbreak, t...  \n",
       "2      [democrat, believe, health, care, human, right, democratsdelivered, help, family, get,...  \n",
       "3      [congratulation, padems, hardwon, victory, elect, pennsylvania, democrat, commit, grow...  \n",
       "4      [heart, go, tyre, nichols, mother, entire, family, tyre, alive, today, justice, must, ...  \n",
       "...                                                                                          ...  \n",
       "84903                                 [pictureperfect, medieval, city, may, show, u, live, good]  \n",
       "84904                            [stun, alpine, landscape, lead, nietzsche, proclaim, god, dead]  \n",
       "84905                                            [write, system, world, use, exclusively, woman]  \n",
       "84906  [steak, spaghetti, ketchup, postwar, japan, yoshoku, become, countrys, defacto, comfor...  \n",
       "84907  [one, country, leastvisited, prefecture, chef, dramatically, sear, bonito, towering, f...  \n",
       "\n",
       "[84105 rows x 8 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a copy of the df, leave the original untouched\n",
    "df_pp = df.copy()\n",
    "preprocessing(df_pp)\n",
    "df_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa028f",
   "metadata": {},
   "source": [
    "Make sure there's no nulls after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5370d06d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:18:35.565258Z",
     "start_time": "2023-02-15T20:18:35.530162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_name         0\n",
       "class             0\n",
       "id                0\n",
       "text              0\n",
       "author_id         0\n",
       "created_at        0\n",
       "RT_user           0\n",
       "text_tokenized    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd60b87",
   "metadata": {},
   "source": [
    "First, let's try to predict the primary interest of the user between our main classifications:\n",
    "- Politics\n",
    "- Sports\n",
    "- Entertainment\n",
    "- Business and finance\n",
    "- Travel\n",
    "- Science / Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ddf6e274",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T19:45:14.558013Z",
     "start_time": "2023-02-15T19:45:14.512920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics                27501\n",
       "Entertainment           23607\n",
       "Sports                  12000\n",
       "Business                 8452\n",
       "Science / Technology     7550\n",
       "Travel                   4995\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.loc[(df_pp['class'] == 'Politics - Conservative') | (df_pp['class'] == 'Politics - Liberal'), 'class'] = 'Politics'\n",
    "df_pp.loc[(df_pp['class'] == 'Music') | (df_pp['class'] == 'TV / movies'), 'class'] = 'Entertainment'\n",
    "df_pp.loc[(df_pp['class'] == 'Business and finance'), 'class'] = 'Business'\n",
    "df_pp['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403e82e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T08:01:47.405009Z",
     "start_time": "2023-02-15T08:01:47.395429Z"
    }
   },
   "source": [
    "Aggregate all text words by account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1825f00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T19:45:25.540433Z",
     "start_time": "2023-02-15T19:45:24.620693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20thcentury</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[titanic, sail, back, theater, valentine, day, weekend, 25th, anniversary, #theacademy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9to5mac</td>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>[9to5toys, last, call, eve, room, homekit, air, quality, monitor, mophie, snap, magsaf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABCNetwork</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[even, betty, think, will, slip, miss, allnew, episode, willtrent, tonight, 109c, abc,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AOC</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acyn</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>thedailybeast</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[favorite, part, jimmykimmel, ask, first, guest, pamela, anderson, ever, meet, mypillo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>travelchannel</td>\n",
       "      <td>Travel</td>\n",
       "      <td>[late, episode, kindredspirits, u, like, miss, it, stream, discoveryplus, amybruni, ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>travelocity</td>\n",
       "      <td>Travel</td>\n",
       "      <td>[sometimes, hard, part, travel, start, pack, process, weve, do, hard, part, ya, tag, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>wbpictures</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[plan, up, up, away, dcstudios, dcu, dccomics, #jamesgunn#, time, grow, up, shazam, fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>wealth</td>\n",
       "      <td>Business</td>\n",
       "      <td>[new, sean, penn, nonprofit, make, name, covidera, hero, current, former, employee, al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_name                 class  \\\n",
       "0      20thcentury         Entertainment   \n",
       "1          9to5mac  Science / Technology   \n",
       "2       ABCNetwork         Entertainment   \n",
       "3              AOC              Politics   \n",
       "4             Acyn              Politics   \n",
       "..             ...                   ...   \n",
       "150  thedailybeast              Politics   \n",
       "151  travelchannel                Travel   \n",
       "152    travelocity                Travel   \n",
       "153     wbpictures         Entertainment   \n",
       "154         wealth              Business   \n",
       "\n",
       "                                                                                text_tokenized  \n",
       "0    [titanic, sail, back, theater, valentine, day, weekend, 25th, anniversary, #theacademy...  \n",
       "1    [9to5toys, last, call, eve, room, homekit, air, quality, monitor, mophie, snap, magsaf...  \n",
       "2    [even, betty, think, will, slip, miss, allnew, episode, willtrent, tonight, 109c, abc,...  \n",
       "3    [excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...  \n",
       "4    [chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...  \n",
       "..                                                                                         ...  \n",
       "150  [favorite, part, jimmykimmel, ask, first, guest, pamela, anderson, ever, meet, mypillo...  \n",
       "151  [late, episode, kindredspirits, u, like, miss, it, stream, discoveryplus, amybruni, ad...  \n",
       "152  [sometimes, hard, part, travel, start, pack, process, weve, do, hard, part, ya, tag, f...  \n",
       "153  [plan, up, up, away, dcstudios, dcu, dccomics, #jamesgunn#, time, grow, up, shazam, fu...  \n",
       "154  [new, sean, penn, nonprofit, make, name, covidera, hero, current, former, employee, al...  \n",
       "\n",
       "[155 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model = df_pp.groupby(['user_name', 'class']).agg({'text_tokenized': 'sum'}).reset_index()\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c911ea77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T19:45:35.818565Z",
     "start_time": "2023-02-15T19:45:35.805517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics                56\n",
       "Entertainment           50\n",
       "Sports                  24\n",
       "Business                10\n",
       "Science / Technology     9\n",
       "Travel                   6\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0dde55a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T19:45:46.472821Z",
     "start_time": "2023-02-15T19:45:46.430690Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>count_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20thcentury</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[titanic, sail, back, theater, valentine, day, weekend, 25th, anniversary, #theacademy...</td>\n",
       "      <td>6348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9to5mac</td>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>[9to5toys, last, call, eve, room, homekit, air, quality, monitor, mophie, snap, magsaf...</td>\n",
       "      <td>8886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABCNetwork</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[even, betty, think, will, slip, miss, allnew, episode, willtrent, tonight, 109c, abc,...</td>\n",
       "      <td>5739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AOC</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...</td>\n",
       "      <td>7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acyn</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...</td>\n",
       "      <td>5426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>thedailybeast</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[favorite, part, jimmykimmel, ask, first, guest, pamela, anderson, ever, meet, mypillo...</td>\n",
       "      <td>7388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>travelchannel</td>\n",
       "      <td>Travel</td>\n",
       "      <td>[late, episode, kindredspirits, u, like, miss, it, stream, discoveryplus, amybruni, ad...</td>\n",
       "      <td>7620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>travelocity</td>\n",
       "      <td>Travel</td>\n",
       "      <td>[sometimes, hard, part, travel, start, pack, process, weve, do, hard, part, ya, tag, f...</td>\n",
       "      <td>10614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>wbpictures</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[plan, up, up, away, dcstudios, dcu, dccomics, #jamesgunn#, time, grow, up, shazam, fu...</td>\n",
       "      <td>6226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>wealth</td>\n",
       "      <td>Business</td>\n",
       "      <td>[new, sean, penn, nonprofit, make, name, covidera, hero, current, former, employee, al...</td>\n",
       "      <td>9750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_name                 class  \\\n",
       "0      20thcentury         Entertainment   \n",
       "1          9to5mac  Science / Technology   \n",
       "2       ABCNetwork         Entertainment   \n",
       "3              AOC              Politics   \n",
       "4             Acyn              Politics   \n",
       "..             ...                   ...   \n",
       "150  thedailybeast              Politics   \n",
       "151  travelchannel                Travel   \n",
       "152    travelocity                Travel   \n",
       "153     wbpictures         Entertainment   \n",
       "154         wealth              Business   \n",
       "\n",
       "                                                                                text_tokenized  \\\n",
       "0    [titanic, sail, back, theater, valentine, day, weekend, 25th, anniversary, #theacademy...   \n",
       "1    [9to5toys, last, call, eve, room, homekit, air, quality, monitor, mophie, snap, magsaf...   \n",
       "2    [even, betty, think, will, slip, miss, allnew, episode, willtrent, tonight, 109c, abc,...   \n",
       "3    [excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...   \n",
       "4    [chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...   \n",
       "..                                                                                         ...   \n",
       "150  [favorite, part, jimmykimmel, ask, first, guest, pamela, anderson, ever, meet, mypillo...   \n",
       "151  [late, episode, kindredspirits, u, like, miss, it, stream, discoveryplus, amybruni, ad...   \n",
       "152  [sometimes, hard, part, travel, start, pack, process, weve, do, hard, part, ya, tag, f...   \n",
       "153  [plan, up, up, away, dcstudios, dcu, dccomics, #jamesgunn#, time, grow, up, shazam, fu...   \n",
       "154  [new, sean, penn, nonprofit, make, name, covidera, hero, current, former, employee, al...   \n",
       "\n",
       "     count_words  \n",
       "0           6348  \n",
       "1           8886  \n",
       "2           5739  \n",
       "3           7371  \n",
       "4           5426  \n",
       "..           ...  \n",
       "150         7388  \n",
       "151         7620  \n",
       "152        10614  \n",
       "153         6226  \n",
       "154         9750  \n",
       "\n",
       "[155 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['count_words'] = df_model['text_tokenized'].apply(len)\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bce39d31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T19:45:46.487540Z",
     "start_time": "2023-02-15T19:45:46.474612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics                0.361290\n",
       "Entertainment           0.322581\n",
       "Sports                  0.154839\n",
       "Business                0.064516\n",
       "Science / Technology    0.058065\n",
       "Travel                  0.038710\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94248e06",
   "metadata": {},
   "source": [
    "Aggregate word count by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6fd79ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T19:46:08.771416Z",
     "start_time": "2023-02-15T19:46:08.743519Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>count_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>110475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>232995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politics</td>\n",
       "      <td>370771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>109941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sports</td>\n",
       "      <td>107750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Travel</td>\n",
       "      <td>64818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  class  count_words\n",
       "0              Business       110475\n",
       "1         Entertainment       232995\n",
       "2              Politics       370771\n",
       "3  Science / Technology       109941\n",
       "4                Sports       107750\n",
       "5                Travel        64818"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_by_class = df_model.groupby(['class']).agg({'count_words': 'sum'}).reset_index()\n",
    "df_model_by_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29944066",
   "metadata": {},
   "source": [
    "### Train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "642a596a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:10:48.802940Z",
     "start_time": "2023-02-15T20:10:48.787793Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_model['text_tokenized']\n",
    "y = df_model['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.35, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a5a7b2",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b66ef",
   "metadata": {},
   "source": [
    "Use Dummy Classifier to predict most frequent label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "131488f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:10:59.057116Z",
     "start_time": "2023-02-15T20:10:59.043137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Politics'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X, y)\n",
    "dummy_clf.predict(X)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29997be",
   "metadata": {},
   "source": [
    "Get accuracy of the dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4b09367e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:11:08.947492Z",
     "start_time": "2023-02-15T20:11:08.933532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36129032258064514"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eff74f",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e532452",
   "metadata": {},
   "source": [
    "Use Tfidfvectorizer to vectorize the tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5d937396",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:16:08.591894Z",
     "start_time": "2023-02-15T20:16:03.769759Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate a vectorizer with max_features=10\n",
    "# (we are using the default token pattern)\n",
    "tfidf = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, \n",
    "                        preprocessor=dummy_fun, token_pattern=None, \n",
    "                        ngram_range=(1,3), min_df=2, max_features=750)\n",
    "\n",
    "# Fit the vectorizer on X_train[\"text\"] and transform it\n",
    "X_train_vectorized = tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e85519",
   "metadata": {},
   "source": [
    "Use a Complement Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "219fb12a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:16:21.822614Z",
     "start_time": "2023-02-15T20:16:21.792566Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natek\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:670: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import relevant class and function\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiate a MultinomialNB classifier\n",
    "baseline_model = ComplementNB()\n",
    "\n",
    "# Evaluate the classifier on X_train_vectorized and y_train\n",
    "baseline_cv = cross_val_score(baseline_model, X_train_vectorized, y_train)\n",
    "baseline_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2661ffb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:16:22.524272Z",
     "start_time": "2023-02-15T20:16:21.824442Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the vectorizer on X_train[\"text\"] and transform it\n",
    "X_test_vectorized = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3bab5ccf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:16:22.556081Z",
     "start_time": "2023-02-15T20:16:22.526054Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natek\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:670: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7818181818181819"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the classifier on X_train_vectorized and y_train\n",
    "baseline_cv = cross_val_score(baseline_model, X_test_vectorized, y_test)\n",
    "baseline_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c2f6c626",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:16:22.572019Z",
     "start_time": "2023-02-15T20:16:22.557962Z"
    }
   },
   "outputs": [],
   "source": [
    "bearer_key = 'AAAAAAAAAAAAAAAAAAAAAAP3lAEAAAAAWiRYIS1QJmco7YZB4oL%2BhLg1R3c%3DmvYmGNwcKhY145AcnvJzFaJlMZ2G7aeovV9VFB5qG9NiNkizEm'\n",
    "\n",
    "def get_tweets(username, class_, number_of_tweets):\n",
    "    # This is the key to use to download the tweets\n",
    "   \n",
    "    client = tweepy.Client(bearer_token=bearer_key)\n",
    "    user_id = client.get_user(username=username).data.id\n",
    "\n",
    "    # Uses the paginator to request as many tweets as we want (paginator makes it possible to download more than 100 at a time\n",
    "    tweets = []\n",
    "    for tweet in tweepy.Paginator(client.get_users_tweets, user_id, tweet_fields=['created_at', 'author_id'],expansions=[''], max_results=100, exclude=['replies']).flatten(limit=number_of_tweets):\n",
    "        # Scrub the text of any non-readable characters\n",
    "        text = \"\".join(i for i in tweet.text if i in string.printable)\n",
    "        # Scrub the text of any newlines\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        # Put the tweet info into a new dictionary\n",
    "        tweets.append({\n",
    "            \"user_name\"  : str(username),\n",
    "            'class'      : str(class_),\n",
    "            \"id\"         : str(tweet.id),\n",
    "            \"text\"       : str(text),\n",
    "            \"author_id\"  : str(tweet.author_id),\n",
    "            \"created_at\" : str(tweet.created_at)\n",
    "        })\n",
    "    return tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2fc53773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:16:23.208694Z",
     "start_time": "2023-02-15T20:16:22.573885Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cryptocom:  Entertainment\n"
     ]
    }
   ],
   "source": [
    "username = 'cryptocom'\n",
    "if username in df['user_name'].unique():\n",
    "    print(\"Error:  User name is in the original dataset. Test a different user.\")\n",
    "else:\n",
    "    tweets = get_tweets(username, 'unknown', 50)\n",
    "    if len(tweets) > 0:\n",
    "        df_new = pd.DataFrame.from_dict(tweets)\n",
    "        preprocessing(df_new)\n",
    "        df_new = df_new.groupby(['user_name', 'class']).agg({'text_tokenized': 'sum'}).reset_index()\n",
    "\n",
    "        baseline_model = ComplementNB()\n",
    "        baseline_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "        tf1_new = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, \n",
    "                                preprocessor=dummy_fun, token_pattern=None, \n",
    "                                ngram_range=(1,3), min_df=2, max_features=750, vocabulary=tfidf.vocabulary_)\n",
    "        df_new_vectorized = tf1_new.fit_transform(df_new['text_tokenized'])\n",
    "\n",
    "        category1 = baseline_model.predict(df_new_vectorized)[0]\n",
    "        print(username, \":  \", category1, sep=\"\")\n",
    "    else:\n",
    "        print('Tweets were not returned.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deff8c1",
   "metadata": {},
   "source": [
    "## Model 2 - Political affiliation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a586fd",
   "metadata": {},
   "source": [
    "### Pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d5da4",
   "metadata": {},
   "source": [
    "**Warning** This code performs all pre-processing, including lemmatization of the tweet text.  As such, it takes a few minutes to run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "089138ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:24:10.721368Z",
     "start_time": "2023-02-15T20:23:42.289487Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\natek/nltk_data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6adff80ea91b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Make a copy of the df, leave the original untouched\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf_pp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_pp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_pp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-d308614d81a2>\u001b[0m in \u001b[0;36mpreprocessing\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mpreprocessing_01_model_specific\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mpreprocessing_02_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mpreprocessing_03_tag_and_lemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-d308614d81a2>\u001b[0m in \u001b[0;36mpreprocessing_03_tag_and_lemmatize\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocessing_03_tag_and_lemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_tokenized'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_tokenized'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_and_lemmatize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4198\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4200\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-d308614d81a2>\u001b[0m in \u001b[0;36mtag_and_lemmatize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtag_and_lemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mnewText\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mnewText\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewText\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[0mnewText\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnewText\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mlemma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \"\"\"\n\u001b[1;32m--> 160\u001b[1;33m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             AP_MODEL_LOC = \"file:\" + str(\n\u001b[1;32m--> 168\u001b[1;33m                 \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"taggers/averaged_perceptron_tagger/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mPICKLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m             )\n\u001b[0;32m    170\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[1;31m# Is the path item a zipfile?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mpath_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    525\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mZipFilePathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Make a copy of the df, leave the original untouched\n",
    "df_pp = df.copy()\n",
    "preprocessing(df_pp)\n",
    "df_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aef800",
   "metadata": {},
   "source": [
    "Make sure there's no nulls after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "360e6441",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:18:35.565258Z",
     "start_time": "2023-02-15T20:18:35.530162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_name         0\n",
       "class             0\n",
       "id                0\n",
       "text              0\n",
       "author_id         0\n",
       "created_at        0\n",
       "RT_user           0\n",
       "text_tokenized    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be5278c",
   "metadata": {},
   "source": [
    "First, let's try to predict the primary interest of the user between our main classifications:\n",
    "- Politics\n",
    "- Sports\n",
    "- Entertainment\n",
    "- Business and finance\n",
    "- Travel\n",
    "- Science / Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "670c0005",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:20:49.078699Z",
     "start_time": "2023-02-15T20:20:49.010880Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'class'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'class'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-9587e307997a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# df_pp.loc[(df_pp['class'] == 'Music') | (df_pp['class'] == 'TV / movies'), 'class'] = 'Entertainment'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# df_pp.loc[(df_pp['class'] == 'Business and finance'), 'class'] = 'Business'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf_pp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 989\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    990\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'class'"
     ]
    }
   ],
   "source": [
    "df_pp = df_pp.loc[(df_pp['class'] == 'Politics - Conservative') | (df_pp['class'] == 'Politics - Liberal'), 'class']\n",
    "# df_pp.loc[(df_pp['class'] == 'Music') | (df_pp['class'] == 'TV / movies'), 'class'] = 'Entertainment'\n",
    "# df_pp.loc[(df_pp['class'] == 'Business and finance'), 'class'] = 'Business'\n",
    "df_pp['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6127d3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T08:01:47.405009Z",
     "start_time": "2023-02-15T08:01:47.395429Z"
    }
   },
   "source": [
    "Aggregate all text words by account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1a62be3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T19:45:25.540433Z",
     "start_time": "2023-02-15T19:45:24.620693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20thcentury</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[titanic, sail, back, theater, valentine, day, weekend, 25th, anniversary, #theacademy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9to5mac</td>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>[9to5toys, last, call, eve, room, homekit, air, quality, monitor, mophie, snap, magsaf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABCNetwork</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[even, betty, think, will, slip, miss, allnew, episode, willtrent, tonight, 109c, abc,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AOC</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acyn</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>thedailybeast</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[favorite, part, jimmykimmel, ask, first, guest, pamela, anderson, ever, meet, mypillo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>travelchannel</td>\n",
       "      <td>Travel</td>\n",
       "      <td>[late, episode, kindredspirits, u, like, miss, it, stream, discoveryplus, amybruni, ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>travelocity</td>\n",
       "      <td>Travel</td>\n",
       "      <td>[sometimes, hard, part, travel, start, pack, process, weve, do, hard, part, ya, tag, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>wbpictures</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[plan, up, up, away, dcstudios, dcu, dccomics, #jamesgunn#, time, grow, up, shazam, fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>wealth</td>\n",
       "      <td>Business</td>\n",
       "      <td>[new, sean, penn, nonprofit, make, name, covidera, hero, current, former, employee, al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_name                 class  \\\n",
       "0      20thcentury         Entertainment   \n",
       "1          9to5mac  Science / Technology   \n",
       "2       ABCNetwork         Entertainment   \n",
       "3              AOC              Politics   \n",
       "4             Acyn              Politics   \n",
       "..             ...                   ...   \n",
       "150  thedailybeast              Politics   \n",
       "151  travelchannel                Travel   \n",
       "152    travelocity                Travel   \n",
       "153     wbpictures         Entertainment   \n",
       "154         wealth              Business   \n",
       "\n",
       "                                                                                text_tokenized  \n",
       "0    [titanic, sail, back, theater, valentine, day, weekend, 25th, anniversary, #theacademy...  \n",
       "1    [9to5toys, last, call, eve, room, homekit, air, quality, monitor, mophie, snap, magsaf...  \n",
       "2    [even, betty, think, will, slip, miss, allnew, episode, willtrent, tonight, 109c, abc,...  \n",
       "3    [excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...  \n",
       "4    [chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...  \n",
       "..                                                                                         ...  \n",
       "150  [favorite, part, jimmykimmel, ask, first, guest, pamela, anderson, ever, meet, mypillo...  \n",
       "151  [late, episode, kindredspirits, u, like, miss, it, stream, discoveryplus, amybruni, ad...  \n",
       "152  [sometimes, hard, part, travel, start, pack, process, weve, do, hard, part, ya, tag, f...  \n",
       "153  [plan, up, up, away, dcstudios, dcu, dccomics, #jamesgunn#, time, grow, up, shazam, fu...  \n",
       "154  [new, sean, penn, nonprofit, make, name, covidera, hero, current, former, employee, al...  \n",
       "\n",
       "[155 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model = df_pp.groupby(['user_name', 'class']).agg({'text_tokenized': 'sum'}).reset_index()\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cbb809d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T19:45:35.818565Z",
     "start_time": "2023-02-15T19:45:35.805517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics                56\n",
       "Entertainment           50\n",
       "Sports                  24\n",
       "Business                10\n",
       "Science / Technology     9\n",
       "Travel                   6\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8cc7f582",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T19:45:46.472821Z",
     "start_time": "2023-02-15T19:45:46.430690Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>count_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20thcentury</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[titanic, sail, back, theater, valentine, day, weekend, 25th, anniversary, #theacademy...</td>\n",
       "      <td>6348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9to5mac</td>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>[9to5toys, last, call, eve, room, homekit, air, quality, monitor, mophie, snap, magsaf...</td>\n",
       "      <td>8886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABCNetwork</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[even, betty, think, will, slip, miss, allnew, episode, willtrent, tonight, 109c, abc,...</td>\n",
       "      <td>5739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AOC</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...</td>\n",
       "      <td>7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acyn</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...</td>\n",
       "      <td>5426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>thedailybeast</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[favorite, part, jimmykimmel, ask, first, guest, pamela, anderson, ever, meet, mypillo...</td>\n",
       "      <td>7388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>travelchannel</td>\n",
       "      <td>Travel</td>\n",
       "      <td>[late, episode, kindredspirits, u, like, miss, it, stream, discoveryplus, amybruni, ad...</td>\n",
       "      <td>7620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>travelocity</td>\n",
       "      <td>Travel</td>\n",
       "      <td>[sometimes, hard, part, travel, start, pack, process, weve, do, hard, part, ya, tag, f...</td>\n",
       "      <td>10614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>wbpictures</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[plan, up, up, away, dcstudios, dcu, dccomics, #jamesgunn#, time, grow, up, shazam, fu...</td>\n",
       "      <td>6226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>wealth</td>\n",
       "      <td>Business</td>\n",
       "      <td>[new, sean, penn, nonprofit, make, name, covidera, hero, current, former, employee, al...</td>\n",
       "      <td>9750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_name                 class  \\\n",
       "0      20thcentury         Entertainment   \n",
       "1          9to5mac  Science / Technology   \n",
       "2       ABCNetwork         Entertainment   \n",
       "3              AOC              Politics   \n",
       "4             Acyn              Politics   \n",
       "..             ...                   ...   \n",
       "150  thedailybeast              Politics   \n",
       "151  travelchannel                Travel   \n",
       "152    travelocity                Travel   \n",
       "153     wbpictures         Entertainment   \n",
       "154         wealth              Business   \n",
       "\n",
       "                                                                                text_tokenized  \\\n",
       "0    [titanic, sail, back, theater, valentine, day, weekend, 25th, anniversary, #theacademy...   \n",
       "1    [9to5toys, last, call, eve, room, homekit, air, quality, monitor, mophie, snap, magsaf...   \n",
       "2    [even, betty, think, will, slip, miss, allnew, episode, willtrent, tonight, 109c, abc,...   \n",
       "3    [excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...   \n",
       "4    [chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...   \n",
       "..                                                                                         ...   \n",
       "150  [favorite, part, jimmykimmel, ask, first, guest, pamela, anderson, ever, meet, mypillo...   \n",
       "151  [late, episode, kindredspirits, u, like, miss, it, stream, discoveryplus, amybruni, ad...   \n",
       "152  [sometimes, hard, part, travel, start, pack, process, weve, do, hard, part, ya, tag, f...   \n",
       "153  [plan, up, up, away, dcstudios, dcu, dccomics, #jamesgunn#, time, grow, up, shazam, fu...   \n",
       "154  [new, sean, penn, nonprofit, make, name, covidera, hero, current, former, employee, al...   \n",
       "\n",
       "     count_words  \n",
       "0           6348  \n",
       "1           8886  \n",
       "2           5739  \n",
       "3           7371  \n",
       "4           5426  \n",
       "..           ...  \n",
       "150         7388  \n",
       "151         7620  \n",
       "152        10614  \n",
       "153         6226  \n",
       "154         9750  \n",
       "\n",
       "[155 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['count_words'] = df_model['text_tokenized'].apply(len)\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "79defdc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T19:45:46.487540Z",
     "start_time": "2023-02-15T19:45:46.474612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics                0.361290\n",
       "Entertainment           0.322581\n",
       "Sports                  0.154839\n",
       "Business                0.064516\n",
       "Science / Technology    0.058065\n",
       "Travel                  0.038710\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56beaee6",
   "metadata": {},
   "source": [
    "Aggregate word count by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8bd55886",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T19:46:08.771416Z",
     "start_time": "2023-02-15T19:46:08.743519Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>count_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>110475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>232995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politics</td>\n",
       "      <td>370771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>109941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sports</td>\n",
       "      <td>107750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Travel</td>\n",
       "      <td>64818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  class  count_words\n",
       "0              Business       110475\n",
       "1         Entertainment       232995\n",
       "2              Politics       370771\n",
       "3  Science / Technology       109941\n",
       "4                Sports       107750\n",
       "5                Travel        64818"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_by_class = df_model.groupby(['class']).agg({'count_words': 'sum'}).reset_index()\n",
    "df_model_by_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b975c2",
   "metadata": {},
   "source": [
    "### Train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7a1954ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:10:48.802940Z",
     "start_time": "2023-02-15T20:10:48.787793Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_model['text_tokenized']\n",
    "y = df_model['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.35, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f864e4",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14248386",
   "metadata": {},
   "source": [
    "Use Dummy Classifier to predict most frequent label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f3d19e99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:10:59.057116Z",
     "start_time": "2023-02-15T20:10:59.043137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Politics'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X, y)\n",
    "dummy_clf.predict(X)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331eeef",
   "metadata": {},
   "source": [
    "Get accuracy of the dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2d92b002",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:11:08.947492Z",
     "start_time": "2023-02-15T20:11:08.933532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36129032258064514"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba0fb89",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e05e3a",
   "metadata": {},
   "source": [
    "Use Tfidfvectorizer to vectorize the tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2a8704d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:16:08.591894Z",
     "start_time": "2023-02-15T20:16:03.769759Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate a vectorizer with max_features=10\n",
    "# (we are using the default token pattern)\n",
    "tfidf = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, \n",
    "                        preprocessor=dummy_fun, token_pattern=None, \n",
    "                        ngram_range=(1,3), min_df=2, max_features=750)\n",
    "\n",
    "# Fit the vectorizer on X_train[\"text\"] and transform it\n",
    "X_train_vectorized = tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a5d1ad",
   "metadata": {},
   "source": [
    "Use a Complement Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7a9559b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:16:21.822614Z",
     "start_time": "2023-02-15T20:16:21.792566Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natek\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:670: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import relevant class and function\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiate a MultinomialNB classifier\n",
    "baseline_model = ComplementNB()\n",
    "\n",
    "# Evaluate the classifier on X_train_vectorized and y_train\n",
    "baseline_cv = cross_val_score(baseline_model, X_train_vectorized, y_train)\n",
    "baseline_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "76d1144e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:16:22.524272Z",
     "start_time": "2023-02-15T20:16:21.824442Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the vectorizer on X_train[\"text\"] and transform it\n",
    "X_test_vectorized = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a3d29208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:16:22.556081Z",
     "start_time": "2023-02-15T20:16:22.526054Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natek\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:670: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7818181818181819"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the classifier on X_train_vectorized and y_train\n",
    "baseline_cv = cross_val_score(baseline_model, X_test_vectorized, y_test)\n",
    "baseline_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "07816252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:16:22.572019Z",
     "start_time": "2023-02-15T20:16:22.557962Z"
    }
   },
   "outputs": [],
   "source": [
    "bearer_key = 'AAAAAAAAAAAAAAAAAAAAAAP3lAEAAAAAWiRYIS1QJmco7YZB4oL%2BhLg1R3c%3DmvYmGNwcKhY145AcnvJzFaJlMZ2G7aeovV9VFB5qG9NiNkizEm'\n",
    "\n",
    "def get_tweets(username, class_, number_of_tweets):\n",
    "    # This is the key to use to download the tweets\n",
    "   \n",
    "    client = tweepy.Client(bearer_token=bearer_key)\n",
    "    user_id = client.get_user(username=username).data.id\n",
    "\n",
    "    # Uses the paginator to request as many tweets as we want (paginator makes it possible to download more than 100 at a time\n",
    "    tweets = []\n",
    "    for tweet in tweepy.Paginator(client.get_users_tweets, user_id, tweet_fields=['created_at', 'author_id'],expansions=[''], max_results=100, exclude=['replies']).flatten(limit=number_of_tweets):\n",
    "        # Scrub the text of any non-readable characters\n",
    "        text = \"\".join(i for i in tweet.text if i in string.printable)\n",
    "        # Scrub the text of any newlines\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        # Put the tweet info into a new dictionary\n",
    "        tweets.append({\n",
    "            \"user_name\"  : str(username),\n",
    "            'class'      : str(class_),\n",
    "            \"id\"         : str(tweet.id),\n",
    "            \"text\"       : str(text),\n",
    "            \"author_id\"  : str(tweet.author_id),\n",
    "            \"created_at\" : str(tweet.created_at)\n",
    "        })\n",
    "    return tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "827eec95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T20:16:23.208694Z",
     "start_time": "2023-02-15T20:16:22.573885Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cryptocom:  Entertainment\n"
     ]
    }
   ],
   "source": [
    "username = 'cryptocom'\n",
    "if username in df['user_name'].unique():\n",
    "    print(\"Error:  User name is in the original dataset. Test a different user.\")\n",
    "else:\n",
    "    tweets = get_tweets(username, 'unknown', 50)\n",
    "    if len(tweets) > 0:\n",
    "        df_new = pd.DataFrame.from_dict(tweets)\n",
    "        preprocessing(df_new)\n",
    "        df_new = df_new.groupby(['user_name', 'class']).agg({'text_tokenized': 'sum'}).reset_index()\n",
    "\n",
    "        baseline_model = ComplementNB()\n",
    "        baseline_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "        tf1_new = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, \n",
    "                                preprocessor=dummy_fun, token_pattern=None, \n",
    "                                ngram_range=(1,3), min_df=2, max_features=750, vocabulary=tfidf.vocabulary_)\n",
    "        df_new_vectorized = tf1_new.fit_transform(df_new['text_tokenized'])\n",
    "\n",
    "        category1 = baseline_model.predict(df_new_vectorized)[0]\n",
    "        print(username, \":  \", category1, sep=\"\")\n",
    "    else:\n",
    "        print('Tweets were not returned.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757e9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56373b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fbc882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b20cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbbc7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205576f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
