{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The goal of this project is to perform \n",
    "\n",
    "The goal of this project is to perform a sentiment analysis of Apple customers, and uncover actionable insight that could be used to optimize a marketing strategy going forward. To achieve this, we built a predictive model using Natural Language Processing (NLP), that could rate the sentiment of a tweet based on its content. At the end of our analysis, we present the findings of our model and provide concrete recommendations as to how Apple could improve its marketing strategy going forward and ultimately increase customer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "Developing an excellent marketing strategy is crucial \n",
    "\n",
    "\n",
    "Developing an excellent marketing strategy is crucial for an organization to consistently achieve positive results. To perform effective marketing, companies need to gain a deep understanding of their customers and uncover what matters to them most. The challenge is figuring out how to gain this insight in an efficient manner, and how to consistently implement meaningful change. Fortunately, machine learning provides us with unique and effective tools to perform customer sentiment analysis and guide long-term decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "\n",
    "For this analysis, I utilized tweet data from ~136K tweets from 605 Twitter accounts that were pulled from the Twitter API.  These accounts were manually selected by me to represent each account class that I am trying to predict.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:28:04.530034Z",
     "start_time": "2023-02-16T12:28:04.515841Z"
    }
   },
   "outputs": [],
   "source": [
    "model1_max_features = 1250\n",
    "model1_test_size = .4\n",
    "\n",
    "model2_max_features = 1250\n",
    "model2_test_size = .4\n",
    "\n",
    "model1_min_df = 3\n",
    "model2_min_df = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports / settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:28:07.945822Z",
     "start_time": "2023-02-16T12:28:04.531803Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\natek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\natek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\natek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "# Twitter import\n",
    "import tweepy\n",
    "\n",
    "# Analysis imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP imports\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "# SKlearn imports\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_colwidth = 90\n",
    "\n",
    "# Downloads (for NLP)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "These are helper functions that assist in the manipulation of tweet strings for pre-processing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:28:07.961740Z",
     "start_time": "2023-02-16T12:28:07.947775Z"
    }
   },
   "outputs": [],
   "source": [
    "def strip_rt_user(text):\n",
    "    if text[0:2] == \"RT\":\n",
    "        colon = text.find(\":\")\n",
    "        return text[colon+1:].lower()\n",
    "    else:\n",
    "        return text.lower()\n",
    "\n",
    "def get_rt_user(text):\n",
    "    if text[0:2] == \"RT\":\n",
    "        colon = text.find(\":\")\n",
    "        user = text[:colon]\n",
    "        at = user.find(\"@\")\n",
    "        return (user[at+1:]).lower()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def addHashTags(text):\n",
    "    return \"#\" + text + \"#\"\n",
    "\n",
    "# Translate nltk POS to wordnet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def remove_characters(text, char_to_remove):\n",
    "    str1 = ''.join(x for x in text if not x in char_to_remove)\n",
    "    return str1\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = remove_characters(text, string.punctuation)\n",
    "    return text\n",
    "\n",
    "def tag_and_lemmatize(text):\n",
    "    newText = text\n",
    "    newText = pos_tag(newText)\n",
    "    newText = [(x[0], get_wordnet_pos(x[1])) for x in newText]\n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "    newText = [(lemma.lemmatize(x[0], x[1])) for x in newText]\n",
    "    return newText\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "# perform all pre-processing on a df\n",
    "def preprocessing(df):\n",
    "    preprocessing_01_model_specific(df)\n",
    "    preprocessing_02_general(df)\n",
    "    preprocessing_03_tag_and_lemmatize(df)\n",
    "    \n",
    "    \n",
    "def preprocessing_01_model_specific(df):\n",
    "    # Copy the RT user name from the text column and put it into a different column.\n",
    "    df['RT_user'] = df['text'].apply(get_rt_user)\n",
    "    df['RT_user'] = df['RT_user'].apply(lambda x: addHashTags(x) if x != \"\" else \"\")\n",
    "\n",
    "    # Pull out the RT user name from the text column\n",
    "    df['text'] = df['text'].apply(strip_rt_user)\n",
    "    \n",
    "def preprocessing_02_general(df):\n",
    "    # Lower case the text tweets\n",
    "    df['text'] = df['text'].str.lower()\n",
    "\n",
    "    # Strip out the meaningless links\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join([n for n in x.split() if n[0:4] != \"http\"]))\n",
    "\n",
    "    # Strip any excess white space\n",
    "    df['text'] = df['text'].apply(lambda x: x.strip())\n",
    "    \n",
    "    # Take out stop words\n",
    "    sw = set(stopwords.words('english'))\n",
    "    sw.update(['amp'])\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join([n for n in x.split() if n not in sw]))\n",
    "\n",
    "    # Remove punctuation\n",
    "    df['text'] = df['text'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "    # Make sure we don't have any random numbers\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join([n for n in x.split() if n.isnumeric() == False]))\n",
    "\n",
    "    # Put together the RT user and the tweet text\n",
    "    df['text'] = df['text'] + \" \" + df['RT_user']\n",
    "\n",
    "    # Make a new column, tokenize the words\n",
    "    df['text_tokenized'] = df['text'].str.split()\n",
    "    \n",
    "    df = df.drop(columns=['id', 'author_id', 'created_at'])\n",
    "    \n",
    "    df['text'] = df['text'].apply(lambda x: np.nan if len(x.strip()) == 0 else x)\n",
    "    df = df.dropna().reset_index(drop=True) \n",
    "\n",
    "    le = LabelEncoder()\n",
    "    df['class_label'] = le.fit_transform(df['class'])\n",
    "    df.head()\n",
    "    \n",
    "def preprocessing_03_tag_and_lemmatize(df):\n",
    "    df['text_tokenized'] = df['text_tokenized'].apply(tag_and_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data collection methods and code is located in a separate notebook linked ([here](notebook_02_data_collection.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory data analysis methods and code is located in a separate notebook linked ([here](notebook_03_eda.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T19:56:38.197672Z",
     "start_time": "2023-02-15T19:56:38.191658Z"
    }
   },
   "source": [
    "## Model 1 - Predict primary interest of user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:33:35.951004Z",
     "start_time": "2023-02-15T05:33:35.933045Z"
    }
   },
   "source": [
    "### Load tweet data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tweet data from file.  Model 1 uses tweet_list2.csv, which is a scaled down version of all tweet data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:28:08.467360Z",
     "start_time": "2023-02-16T12:28:07.993651Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.62e+18</td>\n",
       "      <td>On this day 83 years ago, Democrats Delivered the first Social Security checks ever!  ...</td>\n",
       "      <td>2461810448.0</td>\n",
       "      <td>2023-01-31 21:00:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.62e+18</td>\n",
       "      <td>We must keep our children safe from gun violence. Safe storage of guns saves lives and...</td>\n",
       "      <td>2461810448.0</td>\n",
       "      <td>2023-01-30 18:45:49+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.62e+18</td>\n",
       "      <td>Democrats believe that health care is a human right  and #DemocratsDelivered help for ...</td>\n",
       "      <td>2461810448.0</td>\n",
       "      <td>2023-01-28 21:20:12+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.62e+18</td>\n",
       "      <td>Congratulations @PADems for your hard-won victories electing Pennsylvania Democrats wh...</td>\n",
       "      <td>2461810448.0</td>\n",
       "      <td>2023-01-28 04:00:31+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.62e+18</td>\n",
       "      <td>My heart goes out to Tyre Nichols mother and their entire family. Tyre should be alive...</td>\n",
       "      <td>2461810448.0</td>\n",
       "      <td>2023-01-28 02:15:32+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_name               class        id  \\\n",
       "0  TeamPelosi  Politics - Liberal  1.62e+18   \n",
       "1  TeamPelosi  Politics - Liberal  1.62e+18   \n",
       "2  TeamPelosi  Politics - Liberal  1.62e+18   \n",
       "3  TeamPelosi  Politics - Liberal  1.62e+18   \n",
       "4  TeamPelosi  Politics - Liberal  1.62e+18   \n",
       "\n",
       "                                                                                        text  \\\n",
       "0  On this day 83 years ago, Democrats Delivered the first Social Security checks ever!  ...   \n",
       "1  We must keep our children safe from gun violence. Safe storage of guns saves lives and...   \n",
       "2  Democrats believe that health care is a human right  and #DemocratsDelivered help for ...   \n",
       "3  Congratulations @PADems for your hard-won victories electing Pennsylvania Democrats wh...   \n",
       "4  My heart goes out to Tyre Nichols mother and their entire family. Tyre should be alive...   \n",
       "\n",
       "      author_id                 created_at  \n",
       "0  2461810448.0  2023-01-31 21:00:26+00:00  \n",
       "1  2461810448.0  2023-01-30 18:45:49+00:00  \n",
       "2  2461810448.0  2023-01-28 21:20:12+00:00  \n",
       "3  2461810448.0  2023-01-28 04:00:31+00:00  \n",
       "4  2461810448.0  2023-01-28 02:15:32+00:00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tweets from file\n",
    "tweet_list_file = 'tweet_list2.csv'\n",
    "df = pd.read_csv(tweet_list_file)\n",
    "\n",
    "# Format all series as strings\n",
    "for n in df.columns:\n",
    "    df[n] = df[n].astype(str)\n",
    "\n",
    "# Check out the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for nulls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:28:08.545344Z",
     "start_time": "2023-02-16T12:28:08.470035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101074 entries, 0 to 101073\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   user_name   101074 non-null  object\n",
      " 1   class       101074 non-null  object\n",
      " 2   id          101074 non-null  object\n",
      " 3   text        101074 non-null  object\n",
      " 4   author_id   101074 non-null  object\n",
      " 5   created_at  101074 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- There are no null values, which makes sense because I downloaded this data myself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:28:08.670200Z",
     "start_time": "2023-02-16T12:28:08.549332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- I have some duplicate tweets.  As I noted in the data collection notebook, I must have downloaded some tweets from the same account multiple times while performing the download function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:28:08.888339Z",
     "start_time": "2023-02-16T12:28:08.674152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Duplicates have been deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check class balance at the tweet level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:28:08.904315Z",
     "start_time": "2023-02-16T12:28:08.892377Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Business and finance       21614\n",
       "Science / Technology       15548\n",
       "Politics - Conservative    15500\n",
       "TV / movies                12007\n",
       "Politics - Liberal         12001\n",
       "Sports                     12000\n",
       "Music                      11600\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T06:29:25.411417Z",
     "start_time": "2023-02-15T06:29:25.399488Z"
    }
   },
   "source": [
    "Notes: \n",
    "- It's imbalanced but I'm going to leave it and see if we can still make predictions from the data we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning** This code performs all pre-processing, including lemmatization of the tweet text.  As such, it takes a few minutes to run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:34.303093Z",
     "start_time": "2023-02-16T12:28:08.906310Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a copy of the df, leave the original untouched\n",
    "df_pp = df.copy()\n",
    "preprocessing(df_pp)\n",
    "df_model = df_pp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure there's no nulls after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:34.369708Z",
     "start_time": "2023-02-16T12:30:34.307873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_name         0\n",
       "class             0\n",
       "id                0\n",
       "text              0\n",
       "author_id         0\n",
       "created_at        0\n",
       "RT_user           0\n",
       "text_tokenized    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try to predict the primary interest of the user between our main classifications:\n",
    "- Politics\n",
    "- Sports and entertainment\n",
    "- Business and finance\n",
    "- Science / Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:34.477078Z",
     "start_time": "2023-02-16T12:30:34.373500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sports / Entertainment    35607\n",
       "Politics                  27501\n",
       "Business                  21614\n",
       "Science / Technology      15548\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.loc[(df_pp['class'] == 'Politics - Conservative') | (df_pp['class'] == 'Politics - Liberal'), 'class'] = 'Politics'\n",
    "df_model.loc[(df_pp['class'] == 'Music') | (df_pp['class'] == 'TV / movies') | (df_pp['class'] == 'Sports'), 'class'] = 'Sports / Entertainment'\n",
    "df_model.loc[(df_pp['class'] == 'Business and finance'), 'class'] = 'Business'\n",
    "df_model = df_model.loc[(df_pp['class'] != 'Travel')]\n",
    "\n",
    "df_model['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T08:01:47.405009Z",
     "start_time": "2023-02-15T08:01:47.395429Z"
    }
   },
   "source": [
    "Aggregate all text words by account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:36.439009Z",
     "start_time": "2023-02-16T12:30:34.481066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20thcentury</td>\n",
       "      <td>Sports / Entertainment</td>\n",
       "      <td>[titanic, sail, back, theater, valentine, day, weekend, 25th, anniversary, #theacademy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9to5mac</td>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>[9to5toys, last, call, eve, room, homekit, air, quality, monitor, mophie, snap, magsaf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABCNetwork</td>\n",
       "      <td>Sports / Entertainment</td>\n",
       "      <td>[even, betty, think, will, slip, miss, allnew, episode, willtrent, tonight, 109c, abc,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AOC</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acyn</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>techreview</td>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>[sign, download, daily, dose, whats, emerge, technology, subscribe, spark, newsletter,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[mayorkas, impeach, month, ago, thank, sentedcruz, put, extra, emphasis, issue, #fairi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>thedailybeast</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[favorite, part, jimmykimmel, ask, first, guest, pamela, anderson, ever, meet, mypillo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>wbpictures</td>\n",
       "      <td>Sports / Entertainment</td>\n",
       "      <td>[plan, up, up, away, dcstudios, dcu, dccomics, #jamesgunn#, time, grow, up, shazam, fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>wealth</td>\n",
       "      <td>Business</td>\n",
       "      <td>[new, sean, penn, nonprofit, make, name, covidera, hero, current, former, employee, al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_name                   class  \\\n",
       "0      20thcentury  Sports / Entertainment   \n",
       "1          9to5mac    Science / Technology   \n",
       "2       ABCNetwork  Sports / Entertainment   \n",
       "3              AOC                Politics   \n",
       "4             Acyn                Politics   \n",
       "..             ...                     ...   \n",
       "170     techreview    Science / Technology   \n",
       "171        tedcruz                Politics   \n",
       "172  thedailybeast                Politics   \n",
       "173     wbpictures  Sports / Entertainment   \n",
       "174         wealth                Business   \n",
       "\n",
       "                                                                                text_tokenized  \n",
       "0    [titanic, sail, back, theater, valentine, day, weekend, 25th, anniversary, #theacademy...  \n",
       "1    [9to5toys, last, call, eve, room, homekit, air, quality, monitor, mophie, snap, magsaf...  \n",
       "2    [even, betty, think, will, slip, miss, allnew, episode, willtrent, tonight, 109c, abc,...  \n",
       "3    [excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...  \n",
       "4    [chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...  \n",
       "..                                                                                         ...  \n",
       "170  [sign, download, daily, dose, whats, emerge, technology, subscribe, spark, newsletter,...  \n",
       "171  [mayorkas, impeach, month, ago, thank, sentedcruz, put, extra, emphasis, issue, #fairi...  \n",
       "172  [favorite, part, jimmykimmel, ask, first, guest, pamela, anderson, ever, meet, mypillo...  \n",
       "173  [plan, up, up, away, dcstudios, dcu, dccomics, #jamesgunn#, time, grow, up, shazam, fu...  \n",
       "174  [new, sean, penn, nonprofit, make, name, covidera, hero, current, former, employee, al...  \n",
       "\n",
       "[175 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model = df_model.groupby(['user_name', 'class']).agg({'text_tokenized': 'sum'}).reset_index()\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:36.470823Z",
     "start_time": "2023-02-16T12:30:36.443960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sports / Entertainment    74\n",
       "Politics                  56\n",
       "Business                  26\n",
       "Science / Technology      19\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:36.517254Z",
     "start_time": "2023-02-16T12:30:36.475802Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>count_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20thcentury</td>\n",
       "      <td>Sports / Entertainment</td>\n",
       "      <td>[titanic, sail, back, theater, valentine, day, weekend, 25th, anniversary, #theacademy...</td>\n",
       "      <td>6348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9to5mac</td>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>[9to5toys, last, call, eve, room, homekit, air, quality, monitor, mophie, snap, magsaf...</td>\n",
       "      <td>8886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABCNetwork</td>\n",
       "      <td>Sports / Entertainment</td>\n",
       "      <td>[even, betty, think, will, slip, miss, allnew, episode, willtrent, tonight, 109c, abc,...</td>\n",
       "      <td>5739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AOC</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...</td>\n",
       "      <td>7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acyn</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...</td>\n",
       "      <td>5426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>techreview</td>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>[sign, download, daily, dose, whats, emerge, technology, subscribe, spark, newsletter,...</td>\n",
       "      <td>11933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[mayorkas, impeach, month, ago, thank, sentedcruz, put, extra, emphasis, issue, #fairi...</td>\n",
       "      <td>5211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>thedailybeast</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[favorite, part, jimmykimmel, ask, first, guest, pamela, anderson, ever, meet, mypillo...</td>\n",
       "      <td>7388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>wbpictures</td>\n",
       "      <td>Sports / Entertainment</td>\n",
       "      <td>[plan, up, up, away, dcstudios, dcu, dccomics, #jamesgunn#, time, grow, up, shazam, fu...</td>\n",
       "      <td>6226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>wealth</td>\n",
       "      <td>Business</td>\n",
       "      <td>[new, sean, penn, nonprofit, make, name, covidera, hero, current, former, employee, al...</td>\n",
       "      <td>9750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_name                   class  \\\n",
       "0      20thcentury  Sports / Entertainment   \n",
       "1          9to5mac    Science / Technology   \n",
       "2       ABCNetwork  Sports / Entertainment   \n",
       "3              AOC                Politics   \n",
       "4             Acyn                Politics   \n",
       "..             ...                     ...   \n",
       "170     techreview    Science / Technology   \n",
       "171        tedcruz                Politics   \n",
       "172  thedailybeast                Politics   \n",
       "173     wbpictures  Sports / Entertainment   \n",
       "174         wealth                Business   \n",
       "\n",
       "                                                                                text_tokenized  \\\n",
       "0    [titanic, sail, back, theater, valentine, day, weekend, 25th, anniversary, #theacademy...   \n",
       "1    [9to5toys, last, call, eve, room, homekit, air, quality, monitor, mophie, snap, magsaf...   \n",
       "2    [even, betty, think, will, slip, miss, allnew, episode, willtrent, tonight, 109c, abc,...   \n",
       "3    [excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...   \n",
       "4    [chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...   \n",
       "..                                                                                         ...   \n",
       "170  [sign, download, daily, dose, whats, emerge, technology, subscribe, spark, newsletter,...   \n",
       "171  [mayorkas, impeach, month, ago, thank, sentedcruz, put, extra, emphasis, issue, #fairi...   \n",
       "172  [favorite, part, jimmykimmel, ask, first, guest, pamela, anderson, ever, meet, mypillo...   \n",
       "173  [plan, up, up, away, dcstudios, dcu, dccomics, #jamesgunn#, time, grow, up, shazam, fu...   \n",
       "174  [new, sean, penn, nonprofit, make, name, covidera, hero, current, former, employee, al...   \n",
       "\n",
       "     count_words  \n",
       "0           6348  \n",
       "1           8886  \n",
       "2           5739  \n",
       "3           7371  \n",
       "4           5426  \n",
       "..           ...  \n",
       "170        11933  \n",
       "171         5211  \n",
       "172         7388  \n",
       "173         6226  \n",
       "174         9750  \n",
       "\n",
       "[175 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['count_words'] = df_model['text_tokenized'].apply(len)\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:36.532807Z",
     "start_time": "2023-02-16T12:30:36.519685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sports / Entertainment    0.422857\n",
       "Politics                  0.320000\n",
       "Business                  0.148571\n",
       "Science / Technology      0.108571\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate word count by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:36.594742Z",
     "start_time": "2023-02-16T12:30:36.538641Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>count_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>279610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Politics</td>\n",
       "      <td>370771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>223767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sports / Entertainment</td>\n",
       "      <td>340745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    class  count_words\n",
       "0                Business       279610\n",
       "1                Politics       370771\n",
       "2    Science / Technology       223767\n",
       "3  Sports / Entertainment       340745"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_by_class = df_model.groupby(['class']).agg({'count_words': 'sum'}).reset_index()\n",
    "df_model_by_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:36.610549Z",
     "start_time": "2023-02-16T12:30:36.599501Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_model['text_tokenized']\n",
    "y = df_model['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=model1_test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Dummy Classifier to predict most frequent label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:36.640945Z",
     "start_time": "2023-02-16T12:30:36.616466Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sports / Entertainment'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X, y)\n",
    "dummy_clf.predict(X)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get accuracy of the dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:36.667785Z",
     "start_time": "2023-02-16T12:30:36.646351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4228571428571429"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complement Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Tfidfvectorizer to vectorize the tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:45.396281Z",
     "start_time": "2023-02-16T12:30:36.671323Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate a vectorizer with max_features=10\n",
    "# (we are using the default token pattern)\n",
    "tfidf = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, \n",
    "                        preprocessor=dummy_fun, token_pattern=None, \n",
    "                        ngram_range=(1,3), min_df=model1_min_df, max_features=model1_max_features)\n",
    "\n",
    "# Fit the vectorizer on X_train[\"text\"] and X_test\n",
    "X_train_vectorized = tfidf.fit_transform(X_train)\n",
    "X_test_vectorized = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a Complement Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:45.434669Z",
     "start_time": "2023-02-16T12:30:45.400268Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9714285714285713"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import relevant class and function\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiate a ComplementNB classifier\n",
    "baseline_model = ComplementNB()\n",
    "baseline_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Evaluate the classifier on X_train_vectorized and y_train\n",
    "baseline_cv = cross_val_score(baseline_model, X_train_vectorized, y_train)\n",
    "baseline_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:45.496570Z",
     "start_time": "2023-02-16T12:30:45.447686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9714285714285715"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the classifier on X_train_vectorized and y_train\n",
    "baseline_cv = cross_val_score(baseline_model, X_test_vectorized, y_test)\n",
    "baseline_cv.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save vectorizer and model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:46.103097Z",
     "start_time": "2023-02-16T12:30:45.510058Z"
    }
   },
   "outputs": [],
   "source": [
    "#store the content\n",
    "with open(\"models/model1_tfidf.pkl\", 'wb') as handle:\n",
    "                    pickle.dump(tfidf, handle)\n",
    "\n",
    "with open(\"models/model1_model.pkl\", 'wb') as handle:\n",
    "                    pickle.dump(baseline_model, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - Predict political affiliation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:33:35.951004Z",
     "start_time": "2023-02-15T05:33:35.933045Z"
    }
   },
   "source": [
    "### Load tweet data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tweet data from file.  Load from full tweet file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:47.037271Z",
     "start_time": "2023-02-16T12:30:46.109992Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.62058e+18</td>\n",
       "      <td>Today marks the 83rd anniversary of the first ever #SocialSecurity check, and Republic...</td>\n",
       "      <td>82453460.0</td>\n",
       "      <td>2023-02-01 00:45:11+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.62012e+18</td>\n",
       "      <td>RT @VP: President Biden and I are just getting started. https://t.co/gLmNbpKGAN</td>\n",
       "      <td>82453460.0</td>\n",
       "      <td>2023-01-30 17:46:29+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.62012e+18</td>\n",
       "      <td>RT @RepJeffries: We will never negotiate away the health, safety or economic well-bein...</td>\n",
       "      <td>82453460.0</td>\n",
       "      <td>2023-01-30 17:46:12+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.62012e+18</td>\n",
       "      <td>https://t.co/Ze7ePCUJJ2</td>\n",
       "      <td>82453460.0</td>\n",
       "      <td>2023-01-30 17:45:55+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.62006e+18</td>\n",
       "      <td>https://t.co/ley5hNsz0y https://t.co/RFdTeGXGO1</td>\n",
       "      <td>82453460.0</td>\n",
       "      <td>2023-01-30 14:10:33+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_name               class           id  \\\n",
       "0  BennieGThompson  Politics - Liberal  1.62058e+18   \n",
       "1  BennieGThompson  Politics - Liberal  1.62012e+18   \n",
       "2  BennieGThompson  Politics - Liberal  1.62012e+18   \n",
       "3  BennieGThompson  Politics - Liberal  1.62012e+18   \n",
       "4  BennieGThompson  Politics - Liberal  1.62006e+18   \n",
       "\n",
       "                                                                                        text  \\\n",
       "0  Today marks the 83rd anniversary of the first ever #SocialSecurity check, and Republic...   \n",
       "1            RT @VP: President Biden and I are just getting started. https://t.co/gLmNbpKGAN   \n",
       "2  RT @RepJeffries: We will never negotiate away the health, safety or economic well-bein...   \n",
       "3                                                                    https://t.co/Ze7ePCUJJ2   \n",
       "4                                            https://t.co/ley5hNsz0y https://t.co/RFdTeGXGO1   \n",
       "\n",
       "    author_id                 created_at  \n",
       "0  82453460.0  2023-02-01 00:45:11+00:00  \n",
       "1  82453460.0  2023-01-30 17:46:29+00:00  \n",
       "2  82453460.0  2023-01-30 17:46:12+00:00  \n",
       "3  82453460.0  2023-01-30 17:45:55+00:00  \n",
       "4  82453460.0  2023-01-30 14:10:33+00:00  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tweets from file\n",
    "tweet_list_file = 'tweet_list.csv'\n",
    "df = pd.read_csv(tweet_list_file)\n",
    "\n",
    "# Format all series as strings\n",
    "for n in df.columns:\n",
    "    df[n] = df[n].astype(str)\n",
    "\n",
    "# Check out the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for nulls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:47.175667Z",
     "start_time": "2023-02-16T12:30:47.042090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 136672 entries, 0 to 136671\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   user_name   136672 non-null  object\n",
      " 1   class       136672 non-null  object\n",
      " 2   id          136672 non-null  object\n",
      " 3   text        136672 non-null  object\n",
      " 4   author_id   136672 non-null  object\n",
      " 5   created_at  136672 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 6.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- There are no null values, which makes sense because I downloaded this data myself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:47.422467Z",
     "start_time": "2023-02-16T12:30:47.178833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "878"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- I have some duplicate tweets.  As I noted in the data collection notebook, I must have downloaded some tweets from the same account multiple times while performing the download function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:47.886701Z",
     "start_time": "2023-02-16T12:30:47.426536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Duplicates have been deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check class balance at the tweet level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:47.934405Z",
     "start_time": "2023-02-16T12:30:47.891621Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics - Conservative    31032\n",
       "Politics - Liberal         26998\n",
       "Business and finance       21614\n",
       "Science / Technology       15548\n",
       "TV / movies                12007\n",
       "Sports                     12000\n",
       "Music                      11600\n",
       "Travel                      4995\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T06:29:25.411417Z",
     "start_time": "2023-02-15T06:29:25.399488Z"
    }
   },
   "source": [
    "Notes: \n",
    "- It's imbalanced but I'm going to leave it and see if we can still make predictions from the data we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning** This code performs all pre-processing, including lemmatization of the tweet text.  As such, it takes a few minutes to run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:34:24.733015Z",
     "start_time": "2023-02-16T12:30:47.938398Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a copy of the df, leave the original untouched\n",
    "df_pp = df.copy()\n",
    "preprocessing(df_pp)\n",
    "df_model = df_pp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure there's no nulls after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:34:24.850349Z",
     "start_time": "2023-02-16T12:34:24.739832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_name         0\n",
       "class             0\n",
       "id                0\n",
       "text              0\n",
       "author_id         0\n",
       "created_at        0\n",
       "RT_user           0\n",
       "text_tokenized    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narrow it down to just politics - conservative and politicals - liberal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:34:24.945832Z",
     "start_time": "2023-02-16T12:34:24.855367Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics - Conservative    31032\n",
       "Politics - Liberal         26998\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model = df_model.loc[(df_pp['class'] == 'Politics - Conservative') | (df_model['class'] == 'Politics - Liberal')]\n",
    "df_model['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T08:01:47.405009Z",
     "start_time": "2023-02-15T08:01:47.395429Z"
    }
   },
   "source": [
    "Aggregate all text words by account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:34:25.615154Z",
     "start_time": "2023-02-16T12:34:24.947799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AOC</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>[excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acyn</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>[chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AustinScottGA08</td>\n",
       "      <td>Politics - Conservative</td>\n",
       "      <td>[today, international, holocaust, remembrance, day, please, join, honor, million, peop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BarackObama</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>[along, mourn, tyre, support, family, u, mobilize, last, change, learn, community, rei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>[today, mark, 83rd, anniversary, first, ever, socialsecurity, check, republicans, cele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>staceyabrams</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>[across, south, disability, poverty, yoke, georgia, help, eliminate, hbcs, wait, list,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>stkirsch</td>\n",
       "      <td>Politics - Conservative</td>\n",
       "      <td>[response, joel, smalley, mp, regard, evidence, safety, effectiveness, covid, vaccine,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>Politics - Conservative</td>\n",
       "      <td>[mayorkas, impeach, month, ago, thank, sentedcruz, put, extra, emphasis, issue, #fairi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>thedailybeast</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>[favorite, part, jimmykimmel, ask, first, guest, pamela, anderson, ever, meet, mypillo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>virginiafoxx</td>\n",
       "      <td>Politics - Conservative</td>\n",
       "      <td>[regular, order, restore, people, house, student, reward, hard, work, education, burea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>487 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           user_name                    class  \\\n",
       "0                AOC       Politics - Liberal   \n",
       "1               Acyn       Politics - Liberal   \n",
       "2    AustinScottGA08  Politics - Conservative   \n",
       "3        BarackObama       Politics - Liberal   \n",
       "4    BennieGThompson       Politics - Liberal   \n",
       "..               ...                      ...   \n",
       "482     staceyabrams       Politics - Liberal   \n",
       "483         stkirsch  Politics - Conservative   \n",
       "484          tedcruz  Politics - Conservative   \n",
       "485    thedailybeast       Politics - Liberal   \n",
       "486     virginiafoxx  Politics - Conservative   \n",
       "\n",
       "                                                                                text_tokenized  \n",
       "0    [excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...  \n",
       "1    [chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...  \n",
       "2    [today, international, holocaust, remembrance, day, please, join, honor, million, peop...  \n",
       "3    [along, mourn, tyre, support, family, u, mobilize, last, change, learn, community, rei...  \n",
       "4    [today, mark, 83rd, anniversary, first, ever, socialsecurity, check, republicans, cele...  \n",
       "..                                                                                         ...  \n",
       "482  [across, south, disability, poverty, yoke, georgia, help, eliminate, hbcs, wait, list,...  \n",
       "483  [response, joel, smalley, mp, regard, evidence, safety, effectiveness, covid, vaccine,...  \n",
       "484  [mayorkas, impeach, month, ago, thank, sentedcruz, put, extra, emphasis, issue, #fairi...  \n",
       "485  [favorite, part, jimmykimmel, ask, first, guest, pamela, anderson, ever, meet, mypillo...  \n",
       "486  [regular, order, restore, people, house, student, reward, hard, work, education, burea...  \n",
       "\n",
       "[487 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model = df_model.groupby(['user_name', 'class']).agg({'text_tokenized': 'sum'}).reset_index()\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:34:25.644423Z",
     "start_time": "2023-02-16T12:34:25.617942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics - Conservative    251\n",
       "Politics - Liberal         236\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:34:25.683441Z",
     "start_time": "2023-02-16T12:34:25.650064Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>count_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AOC</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>[excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...</td>\n",
       "      <td>7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acyn</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>[chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...</td>\n",
       "      <td>5426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AustinScottGA08</td>\n",
       "      <td>Politics - Conservative</td>\n",
       "      <td>[today, international, holocaust, remembrance, day, please, join, honor, million, peop...</td>\n",
       "      <td>1262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BarackObama</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>[along, mourn, tyre, support, family, u, mobilize, last, change, learn, community, rei...</td>\n",
       "      <td>9587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>[today, mark, 83rd, anniversary, first, ever, socialsecurity, check, republicans, cele...</td>\n",
       "      <td>938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_name                    class  \\\n",
       "0              AOC       Politics - Liberal   \n",
       "1             Acyn       Politics - Liberal   \n",
       "2  AustinScottGA08  Politics - Conservative   \n",
       "3      BarackObama       Politics - Liberal   \n",
       "4  BennieGThompson       Politics - Liberal   \n",
       "\n",
       "                                                                              text_tokenized  \\\n",
       "0  [excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...   \n",
       "1  [chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...   \n",
       "2  [today, international, holocaust, remembrance, day, please, join, honor, million, peop...   \n",
       "3  [along, mourn, tyre, support, family, u, mobilize, last, change, learn, community, rei...   \n",
       "4  [today, mark, 83rd, anniversary, first, ever, socialsecurity, check, republicans, cele...   \n",
       "\n",
       "   count_words  \n",
       "0         7371  \n",
       "1         5426  \n",
       "2         1262  \n",
       "3         9587  \n",
       "4          938  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['count_words'] = df_model['text_tokenized'].apply(len)\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:34:25.724567Z",
     "start_time": "2023-02-16T12:34:25.692089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics - Conservative    0.5154\n",
       "Politics - Liberal         0.4846\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate word count by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:34:25.759521Z",
     "start_time": "2023-02-16T12:34:25.728546Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>count_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Politics - Conservative</td>\n",
       "      <td>443452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>465009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     class  count_words\n",
       "0  Politics - Conservative       443452\n",
       "1       Politics - Liberal       465009"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_by_class = df_model.groupby(['class']).agg({'count_words': 'sum'}).reset_index()\n",
    "df_model_by_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:34:25.802242Z",
     "start_time": "2023-02-16T12:34:25.772483Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_model['text_tokenized']\n",
    "y = df_model['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=model2_test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Dummy Classifier to predict most frequent label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:34:25.837693Z",
     "start_time": "2023-02-16T12:34:25.812858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Politics - Conservative'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X, y)\n",
    "dummy_clf.predict(X)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get accuracy of the dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:34:25.867007Z",
     "start_time": "2023-02-16T12:34:25.844156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5154004106776181"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complement Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Tfidfvectorizer to vectorize the tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:34:31.049779Z",
     "start_time": "2023-02-16T12:34:25.874852Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate a vectorizer with max_features=10\n",
    "# (we are using the default token pattern)\n",
    "tfidf = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, \n",
    "                        preprocessor=dummy_fun, token_pattern=None, \n",
    "                        ngram_range=(1,3), min_df=model2_min_df, max_features=model2_max_features)\n",
    "\n",
    "# Fit the vectorizer on X_train[\"text\"] and X_test\n",
    "X_train_vectorized = tfidf.fit_transform(X_train)\n",
    "X_test_vectorized = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a Complement Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:34:31.124631Z",
     "start_time": "2023-02-16T12:34:31.051856Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9419053185271771"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import relevant class and function\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiate a MultinomialNB classifier\n",
    "baseline_model = ComplementNB()\n",
    "baseline_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Evaluate the classifier on X_train_vectorized and y_train\n",
    "baseline_cv = cross_val_score(baseline_model, X_train_vectorized, y_train)\n",
    "baseline_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:34:31.200911Z",
     "start_time": "2023-02-16T12:34:31.129584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9589743589743589"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the classifier on X_train_vectorized and y_train\n",
    "baseline_cv = cross_val_score(baseline_model, X_test_vectorized, y_test)\n",
    "baseline_cv.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save vectorizer and model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:34:31.712054Z",
     "start_time": "2023-02-16T12:34:31.204519Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"models/model2_tfidf.pkl\", 'wb') as handle:\n",
    "                    pickle.dump(tfidf, handle)\n",
    "\n",
    "with open(\"models/model2_model.pkl\", 'wb') as handle:\n",
    "                    pickle.dump(baseline_model, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deployment methods and code is located in a separate notebook linked ([here](notebook_04_deployment.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XXXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XXXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XXXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "480px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
