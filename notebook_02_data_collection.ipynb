{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection method\n",
    "- This notebook uses the Tweepy package to download tweets for specified accounts from the Twitter API. In order to use the API, you need your own bearer key, which serves as your authentication into the API.  If you are planning to use the API, put your bearer key into the variable 'bearer_key' in the constants section.\n",
    "- It uses a manually created account list file (accounts.csv) which has all of the Twitter handles to download tweets from, the class to assign to each handle, and how many tweets to get download from each handle.  \n",
    "- The accounts file is important to keep current in order to avoid downloading tweets from the same account multiple times as users of the Twitter API are limited to the number of tweets that can be pulled in a one month time period.  As such, when tweets for a particular handle are downloaded, the tweets are immediately appended to a tweets CSV file (tweet_list.csv) and the accounts file is updated to mark that handle as 'done'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install necessary uncommon packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T15:23:26.740666Z",
     "start_time": "2023-02-22T15:23:24.417086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (4.12.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tweepy) (2.28.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.10)\n"
     ]
    }
   ],
   "source": [
    "# Tweet downloader\n",
    "! pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T15:23:27.552072Z",
     "start_time": "2023-02-22T15:23:26.756294Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import string\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T15:23:27.596646Z",
     "start_time": "2023-02-22T15:23:27.582331Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the account list that we'll be pulling tweets from.  \n",
    "account_list_file = \"data/accounts.csv\"  \n",
    "\n",
    "# This is the file that we'll save tweet data to.\n",
    "tweet_list_file = 'data/tweet_list2.csv'  \n",
    "\n",
    "# This the key needed to download from the API\n",
    "bearer_key = 'AAAAAAAAAAAAAAAAAAAAAAP3lAEAAAAAWiRYIS1QJmco7YZB4oL%2BhLg1R3c%3DmvYmGNwcKhY145AcnvJzFaJlMZ2G7aeovV9VFB5qG9NiNkizEm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T03:18:18.831288Z",
     "start_time": "2023-02-15T03:18:18.341363Z"
    }
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account list management functions\n",
    "\n",
    "These functions manage the list of accounts to pull tweets from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T15:23:27.627832Z",
     "start_time": "2023-02-22T15:23:27.611608Z"
    }
   },
   "outputs": [],
   "source": [
    "# This downloads the account list file (the file that has the accounts we'll pull tweets from)\n",
    "def get_account_list_from_file():\n",
    "    \n",
    "    # Get list of accounts from CSV file\n",
    "    df_accounts = pd.read_csv(account_list_file)\n",
    "\n",
    "    # Create a dataframe from the file contents\n",
    "    df_accounts.columns = [n.strip() for n in df_accounts.columns]\n",
    "    df_accounts['Count_Plan'] = df_accounts['Count_Plan'].astype(int)\n",
    "    df_accounts['Count_Actual'] = df_accounts['Count_Actual'].astype(int)\n",
    "    df_accounts['Done'] = df_accounts['Done'].astype(bool)\n",
    "    \n",
    "    # Return the dataframe\n",
    "    return df_accounts\n",
    "\n",
    "# This function saves the account list.  It's saved after each handle download.  \n",
    "def save_account_list_to_file():\n",
    "    account_list.to_csv(account_list_file, index=False)\n",
    "    \n",
    "# This is to reset the account list file.  Should rarely be used unless we want to restart the downloads.\n",
    "def reset_account_list_done():\n",
    "    account_list['Done'] = False\n",
    "    account_list['Count_Actual'] = 0\n",
    "    save_account_list_to_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T04:19:51.114581Z",
     "start_time": "2023-02-15T04:19:51.102208Z"
    }
   },
   "source": [
    "### Tweet downloading and saving functions\n",
    "\n",
    "These functions perform the actual downloading and saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T15:23:27.658873Z",
     "start_time": "2023-02-22T15:23:27.643944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to request tweets from the twitter API for a specified handle, specified number of tweets, and add the specified class to it\n",
    "# Return the list of tweets\n",
    "\n",
    "def get_tweets(username, class_, number_of_tweets):\n",
    "    # This is the key to use to download the tweets\n",
    "   \n",
    "    client = tweepy.Client(bearer_token=bearer_key)\n",
    "    user_id = client.get_user(username=username).data.id\n",
    "\n",
    "    # Uses the paginator to request as many tweets as we want (paginator makes it possible to download more than 100 at a time\n",
    "    tweets = []\n",
    "    for tweet in tweepy.Paginator(client.get_users_tweets, user_id, tweet_fields=['created_at', 'author_id'],expansions=[''], max_results=100, exclude=['replies']).flatten(limit=number_of_tweets):\n",
    "        # Scrub the text of any non-readable characters\n",
    "        text = \"\".join(i for i in tweet.text if i in string.printable)\n",
    "        # Scrub the text of any newlines\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        # Put the tweet info into a new dictionary\n",
    "        tweets.append({\n",
    "            \"user_name\"  : str(username),\n",
    "            'class'      : str(class_),\n",
    "            \"id\"         : str(tweet.id),\n",
    "            \"text\"       : str(text),\n",
    "            \"author_id\"  : str(tweet.author_id),\n",
    "            \"created_at\" : str(tweet.created_at)\n",
    "        })\n",
    "    return tweets\n",
    "\n",
    "\n",
    "\n",
    "# Function to append newly downloaded tweets to file\n",
    "def append_to_tweet_file(tweets):\n",
    "    field_names = ['user_name','class','id','text','author_id', 'created_at']\n",
    "    \n",
    "    # if the tweet data file doesn't exist, we're starting from scratch.  Make the file and put the headers at the top. \n",
    "    if not os.path.exists(tweet_list_file):\n",
    "        with open(tweet_list_file, 'a') as csv_file:\n",
    "            writer = csv.writer(csv_file, quoting=csv.QUOTE_NONNUMERIC) \n",
    "            writer.writerow(field_names)\n",
    "            \n",
    "    # Append the new data to file\n",
    "    with open(tweet_list_file, 'a') as csv_file:\n",
    "        writer = csv.writer(csv_file, quoting=csv.QUOTE_NONNUMERIC) \n",
    "        for t in tweets:\n",
    "            writer.writerow([t['user_name'], t['class'], t['id'], t['text'], t['author_id'], t['created_at']])\n",
    "\n",
    "# Function to pull the next handle from the accounts file and \n",
    "def get_next_handle():\n",
    "    # Find first handle with a False in 'Done' \n",
    "    next_account = 0\n",
    "    total_accounts = len(account_list)\n",
    "    count = 0\n",
    "    \n",
    "    # Loop through the account list to find the next one that doesn't say 'Done'.  This is the next handle to download.  \n",
    "    for n in range(0, total_accounts):\n",
    "        if account_list.loc[n, 'Done'] == False:\n",
    "            # Found next handle to download.  Break the loop.\n",
    "            next_account = n\n",
    "            break\n",
    "\n",
    "    \n",
    "    # Double check we found a handle that doesn't say Done, and then get the tweets for that handle\n",
    "    if account_list.loc[next_account, 'Done'] == False:\n",
    "        handle_to_get = account_list.loc[next_account,'Twitter handle']\n",
    "        class_assignment = account_list.loc[next_account,'Class']\n",
    "        number_to_get = account_list.loc[next_account,'Count_Plan']\n",
    "        # Print what we are downloading\n",
    "        print(f\"Requesting {next_account+1}/{total_accounts-1}: {handle_to_get}, {class_assignment}, {number_to_get} tweets.  \", end=\"\")\n",
    "\n",
    "        tweetlist = get_tweets(handle_to_get, class_assignment, number_to_get)\n",
    "        count = len(tweetlist)\n",
    "        if count > 0:\n",
    "            # We've got tweets.  Mark it done in the accounts file and save it. \n",
    "            print(f\"  Received: {count} tweets.\")\n",
    "            append_to_tweet_file(tweetlist)\n",
    "            account_list.loc[next_account, \"Done\"] = True\n",
    "            account_list.loc[next_account, \"Count_Actual\"] = count\n",
    "            save_account_list_to_file()\n",
    "\n",
    "    return count  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download tweets\n",
    "\n",
    "Two download methods are provided below.  One to download one handle's tweets (the next handle in the account list that isn't downloaded yet).  One to download a batch of the next 50 handles in the account list.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the next handle's tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T01:15:46.966886Z",
     "start_time": "2023-02-16T01:15:46.953894Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get account list from file\n",
    "account_list = get_account_list_from_file()\n",
    "\n",
    "# Download and save the next handle\n",
    "count = get_next_handle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the next 50 handles' tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:12:03.784521Z",
     "start_time": "2023-02-15T05:12:03.762597Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through the next 50 handles to pull from the account file\n",
    "for n in range(50):\n",
    "    count = get_next_handle()  # Returns the number of tweets downloaded.  If zero, end, something didn't work.  \n",
    "    if count == 0:\n",
    "        break\n",
    "    # Sleep for 1 second and then move on to the next handle.  Give it time to download.\n",
    "    sleep(1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data review\n",
    "\n",
    "Review downloaded data and the account status file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review tweet data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load downloaded tweets from file (assumes the tweet file already has downloaded tweets in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T15:23:32.340787Z",
     "start_time": "2023-02-22T15:23:32.041468Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.620000e+18</td>\n",
       "      <td>On this day 83 years ago, Democrats Delivered ...</td>\n",
       "      <td>2.461810e+09</td>\n",
       "      <td>2023-01-31 21:00:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.620000e+18</td>\n",
       "      <td>We must keep our children safe from gun violen...</td>\n",
       "      <td>2.461810e+09</td>\n",
       "      <td>2023-01-30 18:45:49+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.620000e+18</td>\n",
       "      <td>Democrats believe that health care is a human ...</td>\n",
       "      <td>2.461810e+09</td>\n",
       "      <td>2023-01-28 21:20:12+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.620000e+18</td>\n",
       "      <td>Congratulations @PADems for your hard-won vict...</td>\n",
       "      <td>2.461810e+09</td>\n",
       "      <td>2023-01-28 04:00:31+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.620000e+18</td>\n",
       "      <td>My heart goes out to Tyre Nichols mother and t...</td>\n",
       "      <td>2.461810e+09</td>\n",
       "      <td>2023-01-28 02:15:32+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101069</th>\n",
       "      <td>LinusTech</td>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>1.550000e+18</td>\n",
       "      <td>NEW VIDEO!: who would PAY for this Weird Fan? ...</td>\n",
       "      <td>4.036143e+08</td>\n",
       "      <td>2022-08-02 17:18:49+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101070</th>\n",
       "      <td>LinusTech</td>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>1.550000e+18</td>\n",
       "      <td>Big shoutout to @LongMcQuade for providing us ...</td>\n",
       "      <td>4.036143e+08</td>\n",
       "      <td>2022-08-02 17:13:55+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101071</th>\n",
       "      <td>LinusTech</td>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>1.550000e+18</td>\n",
       "      <td>New Video!!: Should I take back my $225,000?  ...</td>\n",
       "      <td>4.036143e+08</td>\n",
       "      <td>2022-08-01 18:45:12+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101072</th>\n",
       "      <td>LinusTech</td>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>1.550000e+18</td>\n",
       "      <td>we finally got colton a standing desk https://...</td>\n",
       "      <td>4.036143e+08</td>\n",
       "      <td>2022-08-01 16:02:07+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101073</th>\n",
       "      <td>LinusTech</td>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>1.550000e+18</td>\n",
       "      <td>upcoming DLC for GTA 6 https://t.co/jluyfFA5f9</td>\n",
       "      <td>4.036143e+08</td>\n",
       "      <td>2022-08-01 00:00:26+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101074 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_name                 class            id  \\\n",
       "0       TeamPelosi    Politics - Liberal  1.620000e+18   \n",
       "1       TeamPelosi    Politics - Liberal  1.620000e+18   \n",
       "2       TeamPelosi    Politics - Liberal  1.620000e+18   \n",
       "3       TeamPelosi    Politics - Liberal  1.620000e+18   \n",
       "4       TeamPelosi    Politics - Liberal  1.620000e+18   \n",
       "...            ...                   ...           ...   \n",
       "101069   LinusTech  Science / Technology  1.550000e+18   \n",
       "101070   LinusTech  Science / Technology  1.550000e+18   \n",
       "101071   LinusTech  Science / Technology  1.550000e+18   \n",
       "101072   LinusTech  Science / Technology  1.550000e+18   \n",
       "101073   LinusTech  Science / Technology  1.550000e+18   \n",
       "\n",
       "                                                     text     author_id  \\\n",
       "0       On this day 83 years ago, Democrats Delivered ...  2.461810e+09   \n",
       "1       We must keep our children safe from gun violen...  2.461810e+09   \n",
       "2       Democrats believe that health care is a human ...  2.461810e+09   \n",
       "3       Congratulations @PADems for your hard-won vict...  2.461810e+09   \n",
       "4       My heart goes out to Tyre Nichols mother and t...  2.461810e+09   \n",
       "...                                                   ...           ...   \n",
       "101069  NEW VIDEO!: who would PAY for this Weird Fan? ...  4.036143e+08   \n",
       "101070  Big shoutout to @LongMcQuade for providing us ...  4.036143e+08   \n",
       "101071  New Video!!: Should I take back my $225,000?  ...  4.036143e+08   \n",
       "101072  we finally got colton a standing desk https://...  4.036143e+08   \n",
       "101073     upcoming DLC for GTA 6 https://t.co/jluyfFA5f9  4.036143e+08   \n",
       "\n",
       "                       created_at  \n",
       "0       2023-01-31 21:00:26+00:00  \n",
       "1       2023-01-30 18:45:49+00:00  \n",
       "2       2023-01-28 21:20:12+00:00  \n",
       "3       2023-01-28 04:00:31+00:00  \n",
       "4       2023-01-28 02:15:32+00:00  \n",
       "...                           ...  \n",
       "101069  2022-08-02 17:18:49+00:00  \n",
       "101070  2022-08-02 17:13:55+00:00  \n",
       "101071  2022-08-01 18:45:12+00:00  \n",
       "101072  2022-08-01 16:02:07+00:00  \n",
       "101073  2022-08-01 00:00:26+00:00  \n",
       "\n",
       "[101074 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_list_df = pd.read_csv(tweet_list_file)\n",
    "tweet_list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T15:23:43.591717Z",
     "start_time": "2023-02-22T15:23:43.574386Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WSJbusiness       1606\n",
       "appleinsider       850\n",
       "ReutersBiz         850\n",
       "breakingmkts       850\n",
       "financialjuice     850\n",
       "                  ... \n",
       "chrisbrown         400\n",
       "PearlJam           400\n",
       "AppleMusic         400\n",
       "NBCNetwork           7\n",
       "DNC                  1\n",
       "Name: user_name, Length: 175, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_list_df.user_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we've successfully successfully downloaded from all accounts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the accounts status file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the account list and review it to make sure all accounts have been marked 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T15:23:49.395247Z",
     "start_time": "2023-02-22T15:23:49.378691Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    605\n",
       "Name: Done, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "account_list = get_account_list_from_file()\n",
    "account_list.Done.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- All accounts are marked as 'done'.  The tweets totaled ~101K in count from 605 accounts.  \n",
    "\n",
    "Now I'll move on to modeling.  Proceed back to the main notebook. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "281.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
