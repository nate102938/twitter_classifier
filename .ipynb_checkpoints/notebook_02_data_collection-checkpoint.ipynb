{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection method\n",
    "- This notebook uses the Tweepy package to download tweets for specified accounts from the Twitter API. In order to use the API, you need your own bearer key, which serves as your authentication into the API.  If you are planning to use the API, put your bearer key into the variable 'bearer_key' in the constants section.\n",
    "- It uses a manually created account list file (accounts.csv) which has all of the Twitter handles to download tweets from, the class to assign to each handle, and how many tweets to get download from each handle.  \n",
    "- The accounts file is important to keep current in order to avoid downloading tweets from the same account multiple times as users of the Twitter API are limited to the number of tweets that can be pulled in a one month time period.  As such, when tweets for a particular handle are downloaded, the tweets are immediately appended to a tweets CSV file (tweet_list.csv) and the accounts file is updated to mark that handle as 'done'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install necessary uncommon packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:11:55.130845Z",
     "start_time": "2023-02-15T05:11:52.335960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\users\\natek\\anaconda3\\envs\\learn-env\\lib\\site-packages (4.12.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in c:\\users\\natek\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\natek\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tweepy) (2.28.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\users\\natek\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\natek\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\natek\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\natek\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\natek\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "# Tweet downloader\n",
    "! pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:11:55.519983Z",
     "start_time": "2023-02-15T05:11:55.506512Z"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import string\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:11:55.878195Z",
     "start_time": "2023-02-15T05:11:55.864079Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the account list that we'll be pulling tweets from.  \n",
    "account_list_file = \"accounts.csv\"  \n",
    "\n",
    "# This is the file that we'll save tweet data to.\n",
    "tweet_list_file = 'tweet_list.csv'  \n",
    "\n",
    "# This the key needed to download from the API\n",
    "bearer_key = 'AAAAAAAAAAAAAAAAAAAAAAP3lAEAAAAAWiRYIS1QJmco7YZB4oL%2BhLg1R3c%3DmvYmGNwcKhY145AcnvJzFaJlMZ2G7aeovV9VFB5qG9NiNkizEm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T03:18:18.831288Z",
     "start_time": "2023-02-15T03:18:18.341363Z"
    }
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account list management functions\n",
    "\n",
    "These functions manage the list of accounts to pull tweets from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:11:56.265680Z",
     "start_time": "2023-02-15T05:11:56.252245Z"
    }
   },
   "outputs": [],
   "source": [
    "# This downloads the account list file (the file that has the accounts we'll pull tweets from)\n",
    "def get_account_list_from_file():\n",
    "    \n",
    "    # Get list of accounts from CSV file\n",
    "    df_accounts = pd.read_csv(account_list_file)\n",
    "\n",
    "    # Create a dataframe from the file contents\n",
    "    df_accounts.columns = [n.strip() for n in df_accounts.columns]\n",
    "    df_accounts['Count_Plan'] = df_accounts['Count_Plan'].astype(int)\n",
    "    df_accounts['Count_Actual'] = df_accounts['Count_Actual'].astype(int)\n",
    "    df_accounts['Done'] = df_accounts['Done'].astype(bool)\n",
    "    \n",
    "    # Return the dataframe\n",
    "    return df_accounts\n",
    "\n",
    "# This function saves the account list.  It's saved after each handle download.  \n",
    "def save_account_list_to_file():\n",
    "    account_list.to_csv(account_list_file, index=False)\n",
    "    \n",
    "# This is to reset the account list file.  Should rarely be used unless we want to restart the downloads.\n",
    "def reset_account_list_done():\n",
    "    account_list['Done'] = False\n",
    "    account_list['Count_Actual'] = 0\n",
    "    save_account_list_to_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T04:19:51.114581Z",
     "start_time": "2023-02-15T04:19:51.102208Z"
    }
   },
   "source": [
    "### Tweet downloading and saving functions\n",
    "\n",
    "These functions perform the actual downloading and saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:11:56.702810Z",
     "start_time": "2023-02-15T05:11:56.689728Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to request tweets from the twitter API for a specified handle, specified number of tweets, and add the specified class to it\n",
    "# Return the list of tweets\n",
    "\n",
    "def get_tweets(username, class_, number_of_tweets):\n",
    "    # This is the key to use to download the tweets\n",
    "   \n",
    "    client = tweepy.Client(bearer_token=bearer_key)\n",
    "    user_id = client.get_user(username=username).data.id\n",
    "\n",
    "    # Uses the paginator to request as many tweets as we want (paginator makes it possible to download more than 100 at a time\n",
    "    tweets = []\n",
    "    for tweet in tweepy.Paginator(client.get_users_tweets, user_id, tweet_fields=['created_at', 'author_id'],expansions=[''], max_results=100, exclude=['replies']).flatten(limit=number_of_tweets):\n",
    "        # Scrub the text of any non-readable characters\n",
    "        text = \"\".join(i for i in tweet.text if i in string.printable)\n",
    "        # Scrub the text of any newlines\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        # Put the tweet info into a new dictionary\n",
    "        tweets.append({\n",
    "            \"user_name\"  : str(username),\n",
    "            'class'      : str(class_),\n",
    "            \"id\"         : str(tweet.id),\n",
    "            \"text\"       : str(text),\n",
    "            \"author_id\"  : str(tweet.author_id),\n",
    "            \"created_at\" : str(tweet.created_at)\n",
    "        })\n",
    "    return tweets\n",
    "\n",
    "\n",
    "\n",
    "# Function to append newly downloaded tweets to file\n",
    "def append_to_tweet_file(tweets):\n",
    "    field_names = ['user_name','class','id','text','author_id', 'created_at']\n",
    "    \n",
    "    # if the tweet data file doesn't exist, we're starting from scratch.  Make the file and put the headers at the top. \n",
    "    if not os.path.exists(tweet_list_file):\n",
    "        with open(tweet_list_file, 'a') as csv_file:\n",
    "            writer = csv.writer(csv_file, quoting=csv.QUOTE_NONNUMERIC) \n",
    "            writer.writerow(field_names)\n",
    "            \n",
    "    # Append the new data to file\n",
    "    with open(tweet_list_file, 'a') as csv_file:\n",
    "        writer = csv.writer(csv_file, quoting=csv.QUOTE_NONNUMERIC) \n",
    "        for t in tweets:\n",
    "            writer.writerow([t['user_name'], t['class'], t['id'], t['text'], t['author_id'], t['created_at']])\n",
    "\n",
    "# Function to pull the next handle from the accounts file and \n",
    "def get_next_handle():\n",
    "    # Find first handle with a False in 'Done' \n",
    "    next_account = 0\n",
    "    total_accounts = len(account_list)\n",
    "    count = 0\n",
    "    \n",
    "    # Loop through the account list to find the next one that doesn't say 'Done'.  This is the next handle to download.  \n",
    "    for n in range(0, total_accounts):\n",
    "        if account_list.loc[n, 'Done'] == False:\n",
    "            # Found next handle to download.  Break the loop.\n",
    "            next_account = n\n",
    "            break\n",
    "\n",
    "    \n",
    "    # Double check we found a handle that doesn't say Done, and then get the tweets for that handle\n",
    "    if account_list.loc[next_account, 'Done'] == False:\n",
    "        handle_to_get = account_list.loc[next_account,'Twitter handle']\n",
    "        class_assignment = account_list.loc[next_account,'Class']\n",
    "        number_to_get = account_list.loc[next_account,'Count_Plan']\n",
    "        # Print what we are downloading\n",
    "        print(f\"Requesting {next_account+1}/{total_accounts-1}: {handle_to_get}, {class_assignment}, {number_to_get} tweets.  \", end=\"\")\n",
    "\n",
    "        tweetlist = get_tweets(handle_to_get, class_assignment, number_to_get)\n",
    "        count = len(tweetlist)\n",
    "        if count > 0:\n",
    "            # We've got tweets.  Mark it done in the accounts file and save it. \n",
    "            print(f\"  Received: {count} tweets.\")\n",
    "            append_to_tweet_file(tweetlist)\n",
    "            account_list.loc[next_account, \"Done\"] = True\n",
    "            account_list.loc[next_account, \"Count_Actual\"] = count\n",
    "            save_account_list_to_file()\n",
    "\n",
    "    return count  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download tweets\n",
    "\n",
    "Two download methods are provided below.  One to download one handle's tweets (the next handle in the account list that isn't downloaded yet).  One to download a batch of the next 50 handles in the account list.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the next handle's tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:12:01.909125Z",
     "start_time": "2023-02-15T05:12:01.888702Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get account list from file\n",
    "account_list = get_account_list_from_file()\n",
    "\n",
    "# Download and save the next handle\n",
    "count = get_next_handle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the next 50 handles' tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:12:03.784521Z",
     "start_time": "2023-02-15T05:12:03.762597Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through the next 50 handles to pull from the account file\n",
    "for n in range(50):\n",
    "    count = get_next_handle()  # Returns the number of tweets downloaded.  If zero, end, something didn't work.  \n",
    "    if count == 0:\n",
    "        break\n",
    "    # Sleep for 1 second and then move on to the next handle.  Give it time to download.\n",
    "    sleep(1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data review\n",
    "\n",
    "Review downloaded data and the account status file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review tweet data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load downloaded tweets from file (assumes the tweet file already has downloaded tweets in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:12:11.268611Z",
     "start_time": "2023-02-15T05:12:10.952402Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620584010991939584</td>\n",
       "      <td>Today marks the 83rd anniversary of the first ...</td>\n",
       "      <td>82453460</td>\n",
       "      <td>2023-02-01 00:45:11+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620116251749269511</td>\n",
       "      <td>RT @VP: President Biden and I are just getting...</td>\n",
       "      <td>82453460</td>\n",
       "      <td>2023-01-30 17:46:29+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620116182618759168</td>\n",
       "      <td>RT @RepJeffries: We will never negotiate away ...</td>\n",
       "      <td>82453460</td>\n",
       "      <td>2023-01-30 17:46:12+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620116109864357888</td>\n",
       "      <td>https://t.co/Ze7ePCUJJ2</td>\n",
       "      <td>82453460</td>\n",
       "      <td>2023-01-30 17:45:55+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620061909113516036</td>\n",
       "      <td>https://t.co/ley5hNsz0y https://t.co/RFdTeGXGO1</td>\n",
       "      <td>82453460</td>\n",
       "      <td>2023-01-30 14:10:33+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115506</th>\n",
       "      <td>RepLCD</td>\n",
       "      <td>Politics - Conservative</td>\n",
       "      <td>1611786100825006080</td>\n",
       "      <td>It was great to catch up with my friend @RepFe...</td>\n",
       "      <td>1583530102297600000</td>\n",
       "      <td>2023-01-07 18:05:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115507</th>\n",
       "      <td>RepLCD</td>\n",
       "      <td>Politics - Conservative</td>\n",
       "      <td>1611615029660639233</td>\n",
       "      <td>Thank you #OR05 for placing your trust in me t...</td>\n",
       "      <td>1583530102297600000</td>\n",
       "      <td>2023-01-07 06:45:40+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115508</th>\n",
       "      <td>RepLCD</td>\n",
       "      <td>Politics - Conservative</td>\n",
       "      <td>1610791524807081986</td>\n",
       "      <td>A small minority is preventing the House from ...</td>\n",
       "      <td>1583530102297600000</td>\n",
       "      <td>2023-01-05 00:13:21+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115509</th>\n",
       "      <td>RepLCD</td>\n",
       "      <td>Politics - Conservative</td>\n",
       "      <td>1610408428052295681</td>\n",
       "      <td>As I take on the responsibility of serving #OR...</td>\n",
       "      <td>1583530102297600000</td>\n",
       "      <td>2023-01-03 22:51:03+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115510</th>\n",
       "      <td>RepLCD</td>\n",
       "      <td>Politics - Conservative</td>\n",
       "      <td>1610321103058046978</td>\n",
       "      <td>Soon, I'll be sworn in to serve my first term ...</td>\n",
       "      <td>1583530102297600000</td>\n",
       "      <td>2023-01-03 17:04:03+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115511 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              user_name                    class                   id  \\\n",
       "0       BennieGThompson       Politics - Liberal  1620584010991939584   \n",
       "1       BennieGThompson       Politics - Liberal  1620116251749269511   \n",
       "2       BennieGThompson       Politics - Liberal  1620116182618759168   \n",
       "3       BennieGThompson       Politics - Liberal  1620116109864357888   \n",
       "4       BennieGThompson       Politics - Liberal  1620061909113516036   \n",
       "...                 ...                      ...                  ...   \n",
       "115506           RepLCD  Politics - Conservative  1611786100825006080   \n",
       "115507           RepLCD  Politics - Conservative  1611615029660639233   \n",
       "115508           RepLCD  Politics - Conservative  1610791524807081986   \n",
       "115509           RepLCD  Politics - Conservative  1610408428052295681   \n",
       "115510           RepLCD  Politics - Conservative  1610321103058046978   \n",
       "\n",
       "                                                     text  \\\n",
       "0       Today marks the 83rd anniversary of the first ...   \n",
       "1       RT @VP: President Biden and I are just getting...   \n",
       "2       RT @RepJeffries: We will never negotiate away ...   \n",
       "3                                 https://t.co/Ze7ePCUJJ2   \n",
       "4         https://t.co/ley5hNsz0y https://t.co/RFdTeGXGO1   \n",
       "...                                                   ...   \n",
       "115506  It was great to catch up with my friend @RepFe...   \n",
       "115507  Thank you #OR05 for placing your trust in me t...   \n",
       "115508  A small minority is preventing the House from ...   \n",
       "115509  As I take on the responsibility of serving #OR...   \n",
       "115510  Soon, I'll be sworn in to serve my first term ...   \n",
       "\n",
       "                  author_id                 created_at  \n",
       "0                  82453460  2023-02-01 00:45:11+00:00  \n",
       "1                  82453460  2023-01-30 17:46:29+00:00  \n",
       "2                  82453460  2023-01-30 17:46:12+00:00  \n",
       "3                  82453460  2023-01-30 17:45:55+00:00  \n",
       "4                  82453460  2023-01-30 14:10:33+00:00  \n",
       "...                     ...                        ...  \n",
       "115506  1583530102297600000  2023-01-07 18:05:26+00:00  \n",
       "115507  1583530102297600000  2023-01-07 06:45:40+00:00  \n",
       "115508  1583530102297600000  2023-01-05 00:13:21+00:00  \n",
       "115509  1583530102297600000  2023-01-03 22:51:03+00:00  \n",
       "115510  1583530102297600000  2023-01-03 17:04:03+00:00  \n",
       "\n",
       "[115511 rows x 6 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_list_df = pd.read_csv(tweet_list_file)\n",
    "tweet_list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:12:14.402678Z",
     "start_time": "2023-02-15T05:12:14.381523Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WSJbusiness        1606\n",
       "ScienceMagazine     850\n",
       "appleinsider        850\n",
       "YahooFinance        850\n",
       "CNBC                850\n",
       "                   ... \n",
       "repvalhoyle          10\n",
       "JMoylanforGuam       10\n",
       "NBCNetwork            7\n",
       "RepJeffJackson        2\n",
       "DNC                   1\n",
       "Name: user_name, Length: 586, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_list_df.user_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we've successfully successfully downloaded from all accounts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the accounts status file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the account list and review it to make sure all accounts have been marked 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:12:16.083775Z",
     "start_time": "2023-02-15T05:12:16.060819Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    587\n",
       "Name: Done, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "account_list = get_account_list_from_file()\n",
    "account_list.Done.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- All accounts are marked as 'done'.  The tweets totaled 115,511 in count.  It looks like there may be one duplicate in the account file (587) versus the tweet file (586).  I'll delete duplicate data in the main notebook.  \n",
    "\n",
    "Now I'll move on to modeling.  Proceed back to the main notebook. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "480px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
