{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b208415",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The goal of this project is to perform \n",
    "\n",
    "The goal of this project is to perform a sentiment analysis of Apple customers, and uncover actionable insight that could be used to optimize a marketing strategy going forward. To achieve this, we built a predictive model using Natural Language Processing (NLP), that could rate the sentiment of a tweet based on its content. At the end of our analysis, we present the findings of our model and provide concrete recommendations as to how Apple could improve its marketing strategy going forward and ultimately increase customer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6110373",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "Developing an excellent marketing strategy is crucial \n",
    "\n",
    "\n",
    "Developing an excellent marketing strategy is crucial for an organization to consistently achieve positive results. To perform effective marketing, companies need to gain a deep understanding of their customers and uncover what matters to them most. The challenge is figuring out how to gain this insight in an efficient manner, and how to consistently implement meaningful change. Fortunately, machine learning provides us with unique and effective tools to perform customer sentiment analysis and guide long-term decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185ab54",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "\n",
    "For this analysis, I utilized tweet data from 115,511 tweets from 587 Twitter accounts that were pulled from the Twitter API.  These accounts were manually selected by me to represent each account class that I am trying to predict.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e85505",
   "metadata": {},
   "source": [
    "## Imports / settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e93d8154",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:14:22.687387Z",
     "start_time": "2023-02-15T10:14:20.996258Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\natek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\natek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\natek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import string\n",
    "\n",
    "# Analysis imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP imports\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "# SKlearn imports\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_colwidth = 90\n",
    "\n",
    "# Downloads (for NLP)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1b3807",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1753af8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:14:22.703203Z",
     "start_time": "2023-02-15T10:14:22.690233Z"
    }
   },
   "outputs": [],
   "source": [
    "tweet_list_file = 'tweet_list.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba698c",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "These are helper functions that assist in the manipulation of tweet strings for pre-processing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a0066b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:14:22.750169Z",
     "start_time": "2023-02-15T10:14:22.711177Z"
    }
   },
   "outputs": [],
   "source": [
    "def strip_rt_user(text):\n",
    "    if text[0:2] == \"RT\":\n",
    "        colon = text.find(\":\")\n",
    "        return text[colon+1:].lower()\n",
    "    else:\n",
    "        return text.lower()\n",
    "\n",
    "def get_rt_user(text):\n",
    "    if text[0:2] == \"RT\":\n",
    "        colon = text.find(\":\")\n",
    "        user = text[:colon]\n",
    "        at = user.find(\"@\")\n",
    "        return (user[at+1:]).lower()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def addHashTags(text):\n",
    "    return \"#\" + text + \"#\"\n",
    "\n",
    "# Translate nltk POS to wordnet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def remove_characters(text, char_to_remove):\n",
    "    str1 = ''.join(x for x in text if not x in char_to_remove)\n",
    "    return str1\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = remove_characters(text, string.punctuation)\n",
    "    return text\n",
    "\n",
    "def tag_and_lemmatize(text):\n",
    "    newText = text\n",
    "    newText = pos_tag(newText)\n",
    "    newText = [(x[0], get_wordnet_pos(x[1])) for x in newText]\n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "    newText = [(lemma.lemmatize(x[0], x[1])) for x in newText]\n",
    "    return newText\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "# perform all pre-processing on a df\n",
    "def preprocessing(df):\n",
    "    preprocessing_01_model_specific(df)\n",
    "    preprocessing_02_general(df)\n",
    "    preprocessing_03_tag_and_lemmatize(df)\n",
    "    \n",
    "    \n",
    "def preprocessing_01_model_specific(df):\n",
    "    # Copy the RT user name from the text column and put it into a different column.\n",
    "    df['RT_user'] = df['text'].apply(get_rt_user)\n",
    "    df['RT_user'] = df['RT_user'].apply(lambda x: addHashTags(x) if x != \"\" else \"\")\n",
    "\n",
    "    # Pull out the RT user name from the text column\n",
    "    df['text'] = df['text'].apply(strip_rt_user)\n",
    "    \n",
    "def preprocessing_02_general(df):\n",
    "    # Lower case the text tweets\n",
    "    df['text'] = df['text'].str.lower()\n",
    "\n",
    "    # Strip out the meaningless links\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join([n for n in x.split() if n[0:4] != \"http\"]))\n",
    "\n",
    "    # Strip any excess white space\n",
    "    df['text'] = df['text'].apply(lambda x: x.strip())\n",
    "    \n",
    "    # Take out stop words\n",
    "    sw = set(stopwords.words('english'))\n",
    "    sw.update(['amp'])\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join([n for n in x.split() if n not in sw]))\n",
    "\n",
    "    # Remove punctuation\n",
    "    df['text'] = df['text'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "    # Make sure we don't have any random numbers\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join([n for n in x.split() if n.isnumeric() == False]))\n",
    "\n",
    "    # Put together the RT user and the tweet text\n",
    "    df['text'] = df['text'] + \" \" + df['RT_user']\n",
    "\n",
    "    # Make a new column, tokenize the words\n",
    "    df['text_tokenized'] = df['text'].str.split()\n",
    "    \n",
    "    df = df.drop(columns=['id', 'author_id', 'created_at'])\n",
    "    \n",
    "    df['text'] = df['text'].apply(lambda x: np.nan if len(x.strip()) == 0 else x)\n",
    "    df = df.dropna().reset_index(drop=True) \n",
    "\n",
    "    le = LabelEncoder()\n",
    "    df['class_label'] = le.fit_transform(df['class'])\n",
    "    df.head()\n",
    "    \n",
    "def preprocessing_03_tag_and_lemmatize(df):\n",
    "    df['text_tokenized'] = df['text_tokenized'].apply(tag_and_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053180c6",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33d079f",
   "metadata": {},
   "source": [
    "Data collection methods and code is located in a separate notebook linked ([here](notebook_02_data_collection.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234dfbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:33:35.951004Z",
     "start_time": "2023-02-15T05:33:35.933045Z"
    }
   },
   "source": [
    "## Load tweet data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea5171",
   "metadata": {},
   "source": [
    "Load the tweet data from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a0d82a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:14:23.271825Z",
     "start_time": "2023-02-15T10:14:22.752067Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620584010991939584</td>\n",
       "      <td>Today marks the 83rd anniversary of the first ever #SocialSecurity check, and Republic...</td>\n",
       "      <td>82453460</td>\n",
       "      <td>2023-02-01 00:45:11+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620116251749269511</td>\n",
       "      <td>RT @VP: President Biden and I are just getting started. https://t.co/gLmNbpKGAN</td>\n",
       "      <td>82453460</td>\n",
       "      <td>2023-01-30 17:46:29+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620116182618759168</td>\n",
       "      <td>RT @RepJeffries: We will never negotiate away the health, safety or economic well-bein...</td>\n",
       "      <td>82453460</td>\n",
       "      <td>2023-01-30 17:46:12+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620116109864357888</td>\n",
       "      <td>https://t.co/Ze7ePCUJJ2</td>\n",
       "      <td>82453460</td>\n",
       "      <td>2023-01-30 17:45:55+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620061909113516036</td>\n",
       "      <td>https://t.co/ley5hNsz0y https://t.co/RFdTeGXGO1</td>\n",
       "      <td>82453460</td>\n",
       "      <td>2023-01-30 14:10:33+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_name               class                   id  \\\n",
       "0  BennieGThompson  Politics - Liberal  1620584010991939584   \n",
       "1  BennieGThompson  Politics - Liberal  1620116251749269511   \n",
       "2  BennieGThompson  Politics - Liberal  1620116182618759168   \n",
       "3  BennieGThompson  Politics - Liberal  1620116109864357888   \n",
       "4  BennieGThompson  Politics - Liberal  1620061909113516036   \n",
       "\n",
       "                                                                                        text  \\\n",
       "0  Today marks the 83rd anniversary of the first ever #SocialSecurity check, and Republic...   \n",
       "1            RT @VP: President Biden and I are just getting started. https://t.co/gLmNbpKGAN   \n",
       "2  RT @RepJeffries: We will never negotiate away the health, safety or economic well-bein...   \n",
       "3                                                                    https://t.co/Ze7ePCUJJ2   \n",
       "4                                            https://t.co/ley5hNsz0y https://t.co/RFdTeGXGO1   \n",
       "\n",
       "  author_id                 created_at  \n",
       "0  82453460  2023-02-01 00:45:11+00:00  \n",
       "1  82453460  2023-01-30 17:46:29+00:00  \n",
       "2  82453460  2023-01-30 17:46:12+00:00  \n",
       "3  82453460  2023-01-30 17:45:55+00:00  \n",
       "4  82453460  2023-01-30 14:10:33+00:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tweets from file\n",
    "df = pd.read_csv(tweet_list_file)\n",
    "\n",
    "# Format all series as strings\n",
    "for n in df.columns:\n",
    "    df[n] = df[n].astype(str)\n",
    "\n",
    "# Check out the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca018c3b",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01896f6f",
   "metadata": {},
   "source": [
    "**Check for nulls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "181b849b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:14:23.348476Z",
     "start_time": "2023-02-15T10:14:23.274669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 115511 entries, 0 to 115510\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   user_name   115511 non-null  object\n",
      " 1   class       115511 non-null  object\n",
      " 2   id          115511 non-null  object\n",
      " 3   text        115511 non-null  object\n",
      " 4   author_id   115511 non-null  object\n",
      " 5   created_at  115511 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4e0342",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- There are no null values, which makes sense because I downloaded this data myself. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9b3386",
   "metadata": {},
   "source": [
    "**Check for duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e566648",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:14:23.553922Z",
     "start_time": "2023-02-15T10:14:23.350466Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "877"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06927bff",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- I have some duplicate tweets.  As I noted in the data collection notebook, I must have downloaded some tweets from the same account multiple times while performing the download function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc14825b",
   "metadata": {},
   "source": [
    "**Drop duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4725b7bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:14:23.929706Z",
     "start_time": "2023-02-15T10:14:23.555918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b821cb46",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Duplicates have been deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ae9947",
   "metadata": {},
   "source": [
    "## Data review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5040948c",
   "metadata": {},
   "source": [
    "Check class balance at the tweet level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af36859f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:14:23.945611Z",
     "start_time": "2023-02-15T10:14:23.933617Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics - Conservative    31032\n",
       "Politics - Liberal         26998\n",
       "TV / movies                12007\n",
       "Sports                     12000\n",
       "Music                      11600\n",
       "Business and finance        8452\n",
       "Science / Technology        7550\n",
       "Travel                      4995\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ff24de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T06:29:25.411417Z",
     "start_time": "2023-02-15T06:29:25.399488Z"
    }
   },
   "source": [
    "Notes: \n",
    "- It's imbalanced but I'm going to leave it and see if we can still make predictions from the data we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5abf1a9",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f2404",
   "metadata": {},
   "source": [
    "## Pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f371fb",
   "metadata": {},
   "source": [
    "**Warning** This code performs all pre-processing, including lemmatization of the tweet text.  As such, it takes a few minutes to run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42128f6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:16:20.707829Z",
     "start_time": "2023-02-15T10:14:23.948580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>RT_user</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620584010991939584</td>\n",
       "      <td>today marks 83rd anniversary first ever socialsecurity check republicans celebrating t...</td>\n",
       "      <td>82453460</td>\n",
       "      <td>2023-02-01 00:45:11+00:00</td>\n",
       "      <td></td>\n",
       "      <td>[today, mark, 83rd, anniversary, first, ever, socialsecurity, check, republicans, cele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620116251749269511</td>\n",
       "      <td>president biden getting started #vp#</td>\n",
       "      <td>82453460</td>\n",
       "      <td>2023-01-30 17:46:29+00:00</td>\n",
       "      <td>#vp#</td>\n",
       "      <td>[president, biden, get, start, #vp#]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620116182618759168</td>\n",
       "      <td>never negotiate away health safety economic wellbeing american people #repjeffries#</td>\n",
       "      <td>82453460</td>\n",
       "      <td>2023-01-30 17:46:12+00:00</td>\n",
       "      <td>#repjeffries#</td>\n",
       "      <td>[never, negotiate, away, health, safety, economic, wellbeing, american, people, #repje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620116109864357888</td>\n",
       "      <td></td>\n",
       "      <td>82453460</td>\n",
       "      <td>2023-01-30 17:45:55+00:00</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BennieGThompson</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1620061909113516036</td>\n",
       "      <td></td>\n",
       "      <td>82453460</td>\n",
       "      <td>2023-01-30 14:10:33+00:00</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115506</th>\n",
       "      <td>RepLCD</td>\n",
       "      <td>Politics - Conservative</td>\n",
       "      <td>1611786100825006080</td>\n",
       "      <td>great catch friend repfeenstra last night were ready get work amp deliver promises mad...</td>\n",
       "      <td>1583530102297600000</td>\n",
       "      <td>2023-01-07 18:05:26+00:00</td>\n",
       "      <td></td>\n",
       "      <td>[great, catch, friend, repfeenstra, last, night, be, ready, get, work, amp, deliver, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115507</th>\n",
       "      <td>RepLCD</td>\n",
       "      <td>Politics - Conservative</td>\n",
       "      <td>1611615029660639233</td>\n",
       "      <td>thank or05 placing trust represent halls congress solemn promise oregonians carry cons...</td>\n",
       "      <td>1583530102297600000</td>\n",
       "      <td>2023-01-07 06:45:40+00:00</td>\n",
       "      <td></td>\n",
       "      <td>[thank, or05, place, trust, represent, hall, congress, solemn, promise, oregonian, car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115508</th>\n",
       "      <td>RepLCD</td>\n",
       "      <td>Politics - Conservative</td>\n",
       "      <td>1610791524807081986</td>\n",
       "      <td>small minority preventing house work sent do must get economy back track work get cost...</td>\n",
       "      <td>1583530102297600000</td>\n",
       "      <td>2023-01-05 00:13:21+00:00</td>\n",
       "      <td></td>\n",
       "      <td>[small, minority, prevent, house, work, send, do, must, get, economy, back, track, wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115509</th>\n",
       "      <td>RepLCD</td>\n",
       "      <td>Politics - Conservative</td>\n",
       "      <td>1610408428052295681</td>\n",
       "      <td>take responsibility serving or05 im grateful family side</td>\n",
       "      <td>1583530102297600000</td>\n",
       "      <td>2023-01-03 22:51:03+00:00</td>\n",
       "      <td></td>\n",
       "      <td>[take, responsibility, serve, or05, im, grateful, family, side]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115510</th>\n",
       "      <td>RepLCD</td>\n",
       "      <td>Politics - Conservative</td>\n",
       "      <td>1610321103058046978</td>\n",
       "      <td>soon ill sworn serve first term us house representatives truly honor im ready hit grou...</td>\n",
       "      <td>1583530102297600000</td>\n",
       "      <td>2023-01-03 17:04:03+00:00</td>\n",
       "      <td></td>\n",
       "      <td>[soon, ill, swear, serve, first, term, u, house, representative, truly, honor, im, rea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114634 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              user_name                    class                   id  \\\n",
       "0       BennieGThompson       Politics - Liberal  1620584010991939584   \n",
       "1       BennieGThompson       Politics - Liberal  1620116251749269511   \n",
       "2       BennieGThompson       Politics - Liberal  1620116182618759168   \n",
       "3       BennieGThompson       Politics - Liberal  1620116109864357888   \n",
       "4       BennieGThompson       Politics - Liberal  1620061909113516036   \n",
       "...                 ...                      ...                  ...   \n",
       "115506           RepLCD  Politics - Conservative  1611786100825006080   \n",
       "115507           RepLCD  Politics - Conservative  1611615029660639233   \n",
       "115508           RepLCD  Politics - Conservative  1610791524807081986   \n",
       "115509           RepLCD  Politics - Conservative  1610408428052295681   \n",
       "115510           RepLCD  Politics - Conservative  1610321103058046978   \n",
       "\n",
       "                                                                                             text  \\\n",
       "0       today marks 83rd anniversary first ever socialsecurity check republicans celebrating t...   \n",
       "1                                                            president biden getting started #vp#   \n",
       "2             never negotiate away health safety economic wellbeing american people #repjeffries#   \n",
       "3                                                                                                   \n",
       "4                                                                                                   \n",
       "...                                                                                           ...   \n",
       "115506  great catch friend repfeenstra last night were ready get work amp deliver promises mad...   \n",
       "115507  thank or05 placing trust represent halls congress solemn promise oregonians carry cons...   \n",
       "115508  small minority preventing house work sent do must get economy back track work get cost...   \n",
       "115509                                  take responsibility serving or05 im grateful family side    \n",
       "115510  soon ill sworn serve first term us house representatives truly honor im ready hit grou...   \n",
       "\n",
       "                  author_id                 created_at        RT_user  \\\n",
       "0                  82453460  2023-02-01 00:45:11+00:00                  \n",
       "1                  82453460  2023-01-30 17:46:29+00:00           #vp#   \n",
       "2                  82453460  2023-01-30 17:46:12+00:00  #repjeffries#   \n",
       "3                  82453460  2023-01-30 17:45:55+00:00                  \n",
       "4                  82453460  2023-01-30 14:10:33+00:00                  \n",
       "...                     ...                        ...            ...   \n",
       "115506  1583530102297600000  2023-01-07 18:05:26+00:00                  \n",
       "115507  1583530102297600000  2023-01-07 06:45:40+00:00                  \n",
       "115508  1583530102297600000  2023-01-05 00:13:21+00:00                  \n",
       "115509  1583530102297600000  2023-01-03 22:51:03+00:00                  \n",
       "115510  1583530102297600000  2023-01-03 17:04:03+00:00                  \n",
       "\n",
       "                                                                                   text_tokenized  \n",
       "0       [today, mark, 83rd, anniversary, first, ever, socialsecurity, check, republicans, cele...  \n",
       "1                                                            [president, biden, get, start, #vp#]  \n",
       "2       [never, negotiate, away, health, safety, economic, wellbeing, american, people, #repje...  \n",
       "3                                                                                              []  \n",
       "4                                                                                              []  \n",
       "...                                                                                           ...  \n",
       "115506  [great, catch, friend, repfeenstra, last, night, be, ready, get, work, amp, deliver, p...  \n",
       "115507  [thank, or05, place, trust, represent, hall, congress, solemn, promise, oregonian, car...  \n",
       "115508  [small, minority, prevent, house, work, send, do, must, get, economy, back, track, wor...  \n",
       "115509                            [take, responsibility, serve, or05, im, grateful, family, side]  \n",
       "115510  [soon, ill, swear, serve, first, term, u, house, representative, truly, honor, im, rea...  \n",
       "\n",
       "[114634 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a copy of the df, leave the original untouched\n",
    "df_pp = df.copy()\n",
    "preprocessing(df_pp)\n",
    "df_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c57a231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:16:20.739749Z",
     "start_time": "2023-02-15T10:16:20.709622Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BennieGThompson', 'BettyMcCollum04', 'BillPascrell', 'BobbyScott',\n",
       "       'BradSherman', 'Call_Me_Dutch', 'chelliepingree', 'CongBoyle',\n",
       "       'CongressmanRaja', 'CongresswomanSC', 'DonaldNorcross',\n",
       "       'DorisMatsui', 'EleanorNorton', 'FrankPallone', 'GerryConnolly',\n",
       "       'gracenapolitano', 'GuamCongressman', 'Ilhan', 'JacksonLeeTX18',\n",
       "       'JoaquinCastrotx', 'Kilili_Sablan', 'NormaJTorres',\n",
       "       'NydiaVelazquez', 'TeamPelosi', 'AOC', 'staceyabrams', 'ewarren',\n",
       "       'SenWarren', 'JoeBiden', 'KamalaHarris', 'BarackObama',\n",
       "       'HillaryClinton', 'BillClinton', 'WhiteHouse', 'POTUS', 'MSNBC',\n",
       "       'HuffPost', 'CNNPolitics', 'TheAtlantic', 'MotherJones',\n",
       "       'thedailybeast', 'JoyAnnReid', 'DNC', 'Acyn', 'MeidasTouch',\n",
       "       'briantylercohen', 'mmpadellan', 'MarkRuffalo', 'laurenboebert',\n",
       "       'mattgaetz', 'tedcruz', 'RandPaul', 'GOP', 'RNCResearch',\n",
       "       'foxnewspolitics', 'BreitbartNews', 'NEWSMAX', 'TheDCPolitics',\n",
       "       'OANN', 'realDailyWire', 'JesseBWatters', 'TuckerCarlson',\n",
       "       'JudgeJeanine', 'libsoftiktok', 'JackPosobiec', 'bennyjohnson',\n",
       "       'alx', 'stkirsch', 'DineshDSouza', 'GenFlynn', 'atensnut',\n",
       "       'jordanbpeterson', 'MattWalshBlog', 'MrAndyNgo', 'GovRonDeSantis',\n",
       "       'KariLake', 'RSBNetwork', 'kayleighmcenany', 'LifeNewsHQ',\n",
       "       'barstoolsports', 'espn', 'SportsCenter', 'Sportsnet', 'NFL',\n",
       "       'KingJames', 'MLB', 'Cut4', 'MLBONFOX', 'MLBNetwork', 'NHL',\n",
       "       'FIFAcom', 'Cristiano', 'ChampionsLeague', 'neymarjr', 'imVkohli',\n",
       "       'NBA', 'realmadrid', 'FCBarcelona', 'CBSSports', 'NBCSports',\n",
       "       'BleacherReport', 'StephenCurry30', 'KDTrey5', 'ReutersBiz',\n",
       "       'CNBC', 'jimcramer', 'markets', 'WSJmarkets', 'WSJbusiness',\n",
       "       'FinancialTimes', 'wealth', 'BloombergTV', 'YahooFinance',\n",
       "       'PearlJam', 'RollingStones', 'billyjoel', 'justinbieber',\n",
       "       'katyperry', 'rihanna', 'taylorswift13', 'jtimberlake',\n",
       "       'britneyspears', 'shakira', 'ddlovato', 'Jlo', 'BrunoMars',\n",
       "       'Drake', 'MileyCyrus', 'ladygaga', 'NICKIMINAJ', 'Pink',\n",
       "       'selenagomez', 'Eminem', 'onedirection', 'coldplay', 'SnoopDogg',\n",
       "       'iamwill', 'edsheeran', 'chrisbrown', 'Harry_Styles', 'Spotify',\n",
       "       'AppleMusic', 'TomCruise', 'TheRock', 'MarvelStudios',\n",
       "       'DisneyStudios', 'netflix', 'Oprah', 'jimmyfallon',\n",
       "       'JimmyKimmelLive', 'SimonCowell', 'ZacEfron', 'ABCNetwork',\n",
       "       'NBCNetwork', 'CBS', 'wbpictures', 'UniversalPics', '20thcentury',\n",
       "       'SonyPictures', 'DisneyAnimation', 'Pixar', 'ParamountPics',\n",
       "       'RottenTomatoes', 'NASA', 'NatGeo', 'SpaceX', 'ScienceMagazine',\n",
       "       'appleinsider', '9to5mac', 'Microsoft', 'Google', 'GoogleAI',\n",
       "       'travelchannel', 'travelocity', 'NatGeoTravel', 'nytimestravel',\n",
       "       'lonelyplanet', 'BBC_Travel', 'Rep_Magaziner', 'Rep_Peltola',\n",
       "       'Rep_Stansbury', 'RepAdams', 'RepAdamSchiff', 'RepAdamSmith',\n",
       "       'RepAlGreen', 'RepAndreCarson', 'RepAndyKimNJ', 'RepAngieCraig',\n",
       "       'RepAnnaEshoo', 'RepAnnieKuster', 'RepAOC', 'RepAuchincloss',\n",
       "       'RepBarbaraLee', 'RepBarragan', 'RepBeatty', 'RepBeccaB',\n",
       "       'RepBera', 'RepBillFoster', 'repblumenauer', 'RepBonamici',\n",
       "       'RepBonnie', 'RepBowman', 'RepBrianHiggins', 'RepBrownley',\n",
       "       'RepCarbajal', 'RepCardenas', 'RepCartwright', 'RepCasar',\n",
       "       'RepCasten', 'RepChrisPappas', 'RepChuyGarcia', 'RepCicilline',\n",
       "       'repcleaver', 'RepCohen', 'RepColinAllred', 'RepCori',\n",
       "       'RepCuellar', 'RepDanGoldman', 'RepDanKildee', 'RepDannyDavis',\n",
       "       'RepDarrenSoto', 'RepDavids', 'repdavidscott', 'RepDavidTrone',\n",
       "       'RepDean', 'RepDeanPhillips', 'RepDebDingell', 'RepDeborahRoss',\n",
       "       'RepDelBene', 'repdeliaramirez', 'RepDeluzio', 'RepDerekKilmer',\n",
       "       'RepDeSaulnier', 'RepDianaDeGette', 'repdinatitus',\n",
       "       'RepDonaldPayne', 'RepDonBeyer', 'RepDwightEvans', 'RepDWStweets',\n",
       "       'RepEdCase', 'RepEliCrane', 'RepEmiliaSykes', 'RepEricSorensen',\n",
       "       'RepEscobar', 'RepEspaillat', 'RepFletcher', 'RepGaramendi',\n",
       "       'RepGlennIvey', 'RepGolden', 'RepGonzalez', 'RepGraceMeng',\n",
       "       'RepGregLandsman', 'RepGregoryMeeks', 'RepGregStanton',\n",
       "       'RepGwenMoore', 'RepHaleyStevens', 'RepHankJohnson', 'RepHorsford',\n",
       "       'RepHoulahan', 'RepHuffman', 'RepJahanaHayes', 'RepJasmine',\n",
       "       'RepJasonCrow', 'RepJayapal', 'RepJeffJackson', 'RepJeffries',\n",
       "       'RepJerryNadler', 'RepJillTokuda', 'RepJimCosta', 'RepJimmyGomez',\n",
       "       'RepJimmyPanetta', 'RepJoeCourtney', 'RepJoeMorelle',\n",
       "       'RepJoeNeguse', 'RepJohnLarson', 'RepJoshG', 'RepJoshHarder',\n",
       "       'RepJuanVargas', 'RepJudyChu', 'RepKamlagerDove', 'RepKatiePorter',\n",
       "       'RepKClark', 'RepKimSchrier', 'RepKManning', 'RepKweisiMfume',\n",
       "       'RepLBR', 'RepLindaSanchez', 'RepLloydDoggett', 'RepLoisFrankel',\n",
       "       'RepLoriTrahan', 'RepLouCorrea', 'RepLucyMcBath', 'RepMarcyKaptur',\n",
       "       'repmarkpocan', 'RepMarkTakano', 'RepMaxineWaters',\n",
       "       'RepMaxwellFrost', 'RepMcGarvey', 'RepMcGovern', 'RepMenendez',\n",
       "       'RepMGP', 'RepMGS', 'RepMikeLevin', 'RepMikeQuigley',\n",
       "       'RepMoskowitz', 'RepMrvan', 'RepNikema', 'RepNikkiB',\n",
       "       'RepPatRyanNY', 'RepPaulTonko', 'RepPeteAguilar', 'RepPettersen',\n",
       "       'RepPressley', 'RepRashida', 'RepRaskin', 'RepRaulGrijalva',\n",
       "       'RepRaulRuizMD', 'RepRichardNeal', 'RepRickLarsen', 'RepRitchie',\n",
       "       'RepRobertGarcia', 'RepRobinKelly', 'RepRoKhanna',\n",
       "       'RepRubenGallego', 'RepSalinas', 'RepSaraJacobs', 'RepSarbanes',\n",
       "       'RepSchakowsky', 'RepSchneider', 'RepScholten', 'RepScottPeters',\n",
       "       'RepSherrill', 'RepShontelBrown', 'RepShriThanedar', 'RepSlotkin',\n",
       "       'RepSpanberger', 'RepStenyHoyer', 'RepStephenLynch',\n",
       "       'RepStricklandWA', 'RepSummerLee', 'RepSusanWild', 'RepSusieLee',\n",
       "       'RepSwalwell', 'RepSylviaGarcia', 'RepTedLieu', 'RepTeresaLF',\n",
       "       'RepTerriSewell', 'RepThompson', 'RepTroyCarter', 'RepUnderwood',\n",
       "       'repvalhoyle', 'RepVeasey', 'RepWexton', 'RepWileyNickel',\n",
       "       'RepWilson', 'RepYvetteClarke', 'RepZoeLofgren', 'rosadelauro',\n",
       "       'SanfordBishop', 'SpeakerPelosi', 'StaceyPlaskett', 'USRepKCastor',\n",
       "       'USRepKeating', 'ValerieFoushee', 'jahimes', 'Gabe_NM',\n",
       "       'AustinScottGA08', 'boblatta', 'cathymcmorris', 'congbillposey',\n",
       "       'CongMikeSimpson', 'Congressman_JVD', 'CongressmanGT',\n",
       "       'CongressmanKean', 'DesJarlaisTN04', 'DrNealDunnFL2',\n",
       "       'GReschenthaler', 'JayObernolte', 'Jim_Jordan', 'JMoylanforGuam',\n",
       "       'JudgeCarter', 'KenCalvert', 'MarioDB', 'MarkAmodeiNV2',\n",
       "       'michaelcburgess', 'MikeKellyPA', 'PatrickMcHenry', 'PeteSessions',\n",
       "       'Rep_Clyde', 'rep_stevewomack', 'RepAaronBean', 'RepAdrianSmith',\n",
       "       'RepAlexMooney', 'RepAmata', 'RepAndyBarr', 'RepAndyBiggsAZ',\n",
       "       'RepAndyHarrisMD', 'RepAnnWagner', 'RepArmstrongND',\n",
       "       'RepArrington', 'RepAshleyHinson', 'RepBalderson', 'RepBarryMoore',\n",
       "       'RepBenCline', 'RepBentz', 'RepBethVanDuyne', 'RepBice',\n",
       "       'RepBillJohnson', 'RepBlaine', 'RepBlakeMoore', 'RepBobGood',\n",
       "       'RepBoebert', 'RepBost', 'RepBradWenstrup', 'RepBrecheen',\n",
       "       'RepBrianBabin', 'RepBrianFitz', 'RepBrianMast', 'RepBryanSteil',\n",
       "       'RepBuddyCarter', 'RepBurgessOwens', 'RepCarlos', 'RepCarolMiller',\n",
       "       'RepChipRoy', 'RepChrisStewart', 'RepChuck', 'RepCiscomani',\n",
       "       'RepClayHiggins', 'RepCloudTX', 'RepDaleStrong', 'RepDanBishop',\n",
       "       'RepDanCrenshaw', 'repdarrellissa', 'RepDaveJoyce', 'RepDavid',\n",
       "       'RepDavidKustoff', 'RepDavidRouzer', 'RepDavidValadao',\n",
       "       'RepDesposito', 'RepDLamborn', 'RepDLesko', 'RepDonaldsPress',\n",
       "       'RepDonBacon', 'RepDrewFerguson', 'RepDustyJohnson', 'RepEdwards',\n",
       "       'RepEllzey', 'RepEricBurlison', 'RepEzell', 'RepFeenstra',\n",
       "       'repfinstad', 'RepFischbach', 'RepFitzgerald', 'RepFranklin',\n",
       "       'RepFrankLucas', 'RepFrenchHill', 'RepGallagher', 'RepGarbarino',\n",
       "       'RepGarretGraves', 'RepGosar', 'RepGregMurphy', 'RepGregPence',\n",
       "       'RepGregSteube', 'RepGrothman', 'RepGusBilirakis', 'RepGuthrie',\n",
       "       'RepHageman', 'RepHalRogers', 'RepHarshbarger', 'RepHouchin',\n",
       "       'RepHuizenga', 'RepJackBergman', 'RepJamesComer', 'RepJasonSmith',\n",
       "       'RepJeffDuncan', 'RepJenniffer', 'RepJerryCarl', 'RepJimBaird',\n",
       "       'RepJimBanks', 'RepJoeWilson', 'RepJohnCurtis', 'RepJohnJoyce',\n",
       "       'RepJohnRose', 'repjulialetlow', 'RepKatCammack', 'RepKayGranger',\n",
       "       'RepKenBuck', 'repkevinhern', 'RepKiley', 'RepLaHood', 'RepLaLota',\n",
       "       'RepLaMalfa', 'RepLangworthy', 'RepLarryBucshon', 'RepLaTurner',\n",
       "       'RepLaurelLee', 'RepLisaMcClain', 'RepLoudermilk', 'RepLuna',\n",
       "       'RepMalliotakis', 'RepMann', 'RepMariaSalazar', 'RepMarkAlford',\n",
       "       'RepMarkGreen', 'RepMaryMiller', 'RepMattGaetz', 'RepMcCaul',\n",
       "       'RepMcClintock', 'RepMcCormick', 'RepMeuser', 'RepMGriffith',\n",
       "       'RepMichaelGuest', 'RepMikeCarey', 'RepMikeCollins',\n",
       "       'RepMikeGarcia', 'RepMikeJohnson', 'RepMikeLawler',\n",
       "       'RepMikeRogersAL', 'RepMikeTurner', 'RepMMM', 'RepMolinaroNY19',\n",
       "       'RepMonicaDLC', 'RepMoolenaar', 'RepMTG', 'RepNancyMace',\n",
       "       'RepNateMoran', 'RepNewhouse', 'RepOgles', 'RepPatFallon',\n",
       "       'RepPeteStauber', 'RepPfluger', 'RepRalphNorman', 'RepRichHudson',\n",
       "       'RepRickAllen', 'RepRickCrawford', 'RepRonEstes',\n",
       "       'RepRonnyJackson', 'RepRosendale', 'RepRudyYakym', 'RepRussellFry',\n",
       "       'RepRussFulcher', 'RepRutherfordFL', 'RepRWilliams',\n",
       "       'RepRyanZinke', 'RepSamGraves', 'RepSantosNY03', 'RepScottPerry',\n",
       "       'RepSmucker', 'RepSpartz', 'RepSteel', 'RepStefanik', 'RepTenney',\n",
       "       'RepThomasMassie', 'RepTiffany', 'RepTimBurchett', 'RepTimmons',\n",
       "       'RepTonyGonzales', 'RepTrentKelly', 'RepTroyNehls', 'RepVanOrden',\n",
       "       'RepWalberg', 'RepWaltzPress', 'RepWebster', 'RepWesleyHunt',\n",
       "       'RepWesterman', 'RepWilliams', 'RepYoungKim', 'Robert_Aderholt',\n",
       "       'RobWittman', 'RodneyDavis', 'SpeakerMcCarthy', 'SteveScalise',\n",
       "       'TomColeOK04', 'TXRandy14', 'USRepGaryPalmer', 'USRepMikeFlood',\n",
       "       'VernBuchanan', 'virginiafoxx', 'WarrenDavidson', 'RepDuarteCA13',\n",
       "       'ZachNunn', 'Lancegooden', 'JohnJamesMI', 'RepMaxMiller',\n",
       "       'RepKeithSelf', 'RepLCD'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.user_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9fd77e",
   "metadata": {},
   "source": [
    "Make sure there's no nulls after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a132e03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:16:20.802324Z",
     "start_time": "2023-02-15T10:16:20.752453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_name         0\n",
       "class             0\n",
       "id                0\n",
       "text              0\n",
       "author_id         0\n",
       "created_at        0\n",
       "RT_user           0\n",
       "text_tokenized    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd60b87",
   "metadata": {},
   "source": [
    "First, let's try to predict the primary interest of the user between our main classifications:\n",
    "- Politics\n",
    "- Sports\n",
    "- TV / movies\n",
    "- Business and finance\n",
    "- Music\n",
    "- Travel\n",
    "- Science / Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddf6e274",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:16:20.848411Z",
     "start_time": "2023-02-15T10:16:20.814286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics                58030\n",
       "TV / movies             12007\n",
       "Sports                  12000\n",
       "Music                   11600\n",
       "Business and finance     8452\n",
       "Science / Technology     7550\n",
       "Travel                   4995\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.loc[(df_pp['class'] == 'Politics - Conservative') | (df_pp['class'] == 'Politics - Liberal'), 'class'] = 'Politics'\n",
    "df_pp['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1d01bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T08:01:47.405009Z",
     "start_time": "2023-02-15T08:01:47.395429Z"
    }
   },
   "source": [
    "Aggregate all text words by account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1825f00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:16:21.852542Z",
     "start_time": "2023-02-15T10:16:20.864156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20thcentury</td>\n",
       "      <td>TV / movies</td>\n",
       "      <td>[titanic, sail, back, theater, valentine, day, weekend, 25th, anniversary, #theacademy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9to5mac</td>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>[9to5toys, last, call, eve, room, homekit, air, quality, monitor, mophie, snap, magsaf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABCNetwork</td>\n",
       "      <td>TV / movies</td>\n",
       "      <td>[even, betty, think, will, slip, miss, allnew, episode, willtrent, tonight, 109c, abc,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AOC</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acyn</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>travelchannel</td>\n",
       "      <td>Travel</td>\n",
       "      <td>[late, episode, kindredspirits, u, like, miss, it, stream, discoveryplus, amybruni, ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>travelocity</td>\n",
       "      <td>Travel</td>\n",
       "      <td>[sometimes, hard, part, travel, start, pack, process, weve, do, hard, part, ya, tag, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>virginiafoxx</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[regular, order, restore, people, house, student, reward, hard, work, education, burea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>wbpictures</td>\n",
       "      <td>TV / movies</td>\n",
       "      <td>[plan, up, up, away, dcstudios, dcu, dccomics, #jamesgunn#, time, grow, up, shazam, fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>wealth</td>\n",
       "      <td>Business and finance</td>\n",
       "      <td>[new, sean, penn, nonprofit, make, name, covidera, hero, current, former, employee, al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>586 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_name                 class  \\\n",
       "0      20thcentury           TV / movies   \n",
       "1          9to5mac  Science / Technology   \n",
       "2       ABCNetwork           TV / movies   \n",
       "3              AOC              Politics   \n",
       "4             Acyn              Politics   \n",
       "..             ...                   ...   \n",
       "581  travelchannel                Travel   \n",
       "582    travelocity                Travel   \n",
       "583   virginiafoxx              Politics   \n",
       "584     wbpictures           TV / movies   \n",
       "585         wealth  Business and finance   \n",
       "\n",
       "                                                                                text_tokenized  \n",
       "0    [titanic, sail, back, theater, valentine, day, weekend, 25th, anniversary, #theacademy...  \n",
       "1    [9to5toys, last, call, eve, room, homekit, air, quality, monitor, mophie, snap, magsaf...  \n",
       "2    [even, betty, think, will, slip, miss, allnew, episode, willtrent, tonight, 109c, abc,...  \n",
       "3    [excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...  \n",
       "4    [chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...  \n",
       "..                                                                                         ...  \n",
       "581  [late, episode, kindredspirits, u, like, miss, it, stream, discoveryplus, amybruni, ad...  \n",
       "582  [sometimes, hard, part, travel, start, pack, process, weve, do, hard, part, ya, tag, f...  \n",
       "583  [regular, order, restore, people, house, student, reward, hard, work, education, burea...  \n",
       "584  [plan, up, up, away, dcstudios, dcu, dccomics, #jamesgunn#, time, grow, up, shazam, fu...  \n",
       "585  [new, sean, penn, nonprofit, make, name, covidera, hero, current, former, employee, al...  \n",
       "\n",
       "[586 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model = df_pp.groupby(['user_name', 'class']).agg({'text_tokenized': 'sum'}).reset_index()\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c911ea77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:16:21.867235Z",
     "start_time": "2023-02-15T10:16:21.855267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics                487\n",
       "Music                    29\n",
       "Sports                   24\n",
       "TV / movies              21\n",
       "Business and finance     10\n",
       "Science / Technology      9\n",
       "Travel                    6\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dde55a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:16:21.915106Z",
     "start_time": "2023-02-15T10:16:21.878215Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>count_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20thcentury</td>\n",
       "      <td>TV / movies</td>\n",
       "      <td>[titanic, sail, back, theater, valentine, day, weekend, 25th, anniversary, #theacademy...</td>\n",
       "      <td>6348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9to5mac</td>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>[9to5toys, last, call, eve, room, homekit, air, quality, monitor, mophie, snap, magsaf...</td>\n",
       "      <td>8886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABCNetwork</td>\n",
       "      <td>TV / movies</td>\n",
       "      <td>[even, betty, think, will, slip, miss, allnew, episode, willtrent, tonight, 109c, abc,...</td>\n",
       "      <td>5739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AOC</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...</td>\n",
       "      <td>7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acyn</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...</td>\n",
       "      <td>5426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>travelchannel</td>\n",
       "      <td>Travel</td>\n",
       "      <td>[late, episode, kindredspirits, u, like, miss, it, stream, discoveryplus, amybruni, ad...</td>\n",
       "      <td>7620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>travelocity</td>\n",
       "      <td>Travel</td>\n",
       "      <td>[sometimes, hard, part, travel, start, pack, process, weve, do, hard, part, ya, tag, f...</td>\n",
       "      <td>10614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>virginiafoxx</td>\n",
       "      <td>Politics</td>\n",
       "      <td>[regular, order, restore, people, house, student, reward, hard, work, education, burea...</td>\n",
       "      <td>746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>wbpictures</td>\n",
       "      <td>TV / movies</td>\n",
       "      <td>[plan, up, up, away, dcstudios, dcu, dccomics, #jamesgunn#, time, grow, up, shazam, fu...</td>\n",
       "      <td>6226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>wealth</td>\n",
       "      <td>Business and finance</td>\n",
       "      <td>[new, sean, penn, nonprofit, make, name, covidera, hero, current, former, employee, al...</td>\n",
       "      <td>9750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>586 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_name                 class  \\\n",
       "0      20thcentury           TV / movies   \n",
       "1          9to5mac  Science / Technology   \n",
       "2       ABCNetwork           TV / movies   \n",
       "3              AOC              Politics   \n",
       "4             Acyn              Politics   \n",
       "..             ...                   ...   \n",
       "581  travelchannel                Travel   \n",
       "582    travelocity                Travel   \n",
       "583   virginiafoxx              Politics   \n",
       "584     wbpictures           TV / movies   \n",
       "585         wealth  Business and finance   \n",
       "\n",
       "                                                                                text_tokenized  \\\n",
       "0    [titanic, sail, back, theater, valentine, day, weekend, 25th, anniversary, #theacademy...   \n",
       "1    [9to5toys, last, call, eve, room, homekit, air, quality, monitor, mophie, snap, magsaf...   \n",
       "2    [even, betty, think, will, slip, miss, allnew, episode, willtrent, tonight, 109c, abc,...   \n",
       "3    [excite, humble, share, even, select, serve, repraskins, house, oversight, committee, ...   \n",
       "4    [chad, comer, appear, coown, property, james, comer, receive, small, amount, covid, mo...   \n",
       "..                                                                                         ...   \n",
       "581  [late, episode, kindredspirits, u, like, miss, it, stream, discoveryplus, amybruni, ad...   \n",
       "582  [sometimes, hard, part, travel, start, pack, process, weve, do, hard, part, ya, tag, f...   \n",
       "583  [regular, order, restore, people, house, student, reward, hard, work, education, burea...   \n",
       "584  [plan, up, up, away, dcstudios, dcu, dccomics, #jamesgunn#, time, grow, up, shazam, fu...   \n",
       "585  [new, sean, penn, nonprofit, make, name, covidera, hero, current, former, employee, al...   \n",
       "\n",
       "     count_words  \n",
       "0           6348  \n",
       "1           8886  \n",
       "2           5739  \n",
       "3           7371  \n",
       "4           5426  \n",
       "..           ...  \n",
       "581         7620  \n",
       "582        10614  \n",
       "583          746  \n",
       "584         6226  \n",
       "585         9750  \n",
       "\n",
       "[586 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['count_words'] = df_model['text_tokenized'].apply(len)\n",
    "df_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39912f",
   "metadata": {},
   "source": [
    "Aggregate word count by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6fd79ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:19:50.347467Z",
     "start_time": "2023-02-15T10:19:50.323711Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>count_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business and finance</td>\n",
       "      <td>110475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Music</td>\n",
       "      <td>108593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politics</td>\n",
       "      <td>908577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Science / Technology</td>\n",
       "      <td>109941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sports</td>\n",
       "      <td>107750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TV / movies</td>\n",
       "      <td>124402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Travel</td>\n",
       "      <td>64818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  class  count_words\n",
       "0  Business and finance       110475\n",
       "1                 Music       108593\n",
       "2              Politics       908577\n",
       "3  Science / Technology       109941\n",
       "4                Sports       107750\n",
       "5           TV / movies       124402\n",
       "6                Travel        64818"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_by_class = df_model.groupby(['class']).agg({'count_words': 'sum'}).reset_index()\n",
    "df_model_by_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb812f",
   "metadata": {},
   "source": [
    "## Train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "642a596a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:23:44.451277Z",
     "start_time": "2023-02-15T10:23:44.432247Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_model['text_tokenized']\n",
    "y = df_model['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.50, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "893e4f1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:23:46.749120Z",
     "start_time": "2023-02-15T10:23:46.736152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373    [repmoolenaar, recently, sponsor, legislation, would, repeal, unobligated, balance, ap...\n",
       "425    [year, ago, social, security, issue, first, check, housedemocrats, keep, fight, protec...\n",
       "437    [state, public, utility, commission, begin, investigation, lead, technical, issue, off...\n",
       "37     [the, family, have, make, rich, friend, choose, make, family, thing, youll, never, fee...\n",
       "526    [year, ago, today, first, ever, socialsecurity, check, go, out, republican, celebrate,...\n",
       "                                                 ...                                            \n",
       "486    [history, make, chair, cathymcmorris, rodgers, first, woman, chair, energy, commerce, ...\n",
       "563    [america, must, stand, ally, unequivocally, defend, israel, right, defend, existential...\n",
       "516    [break, true, vote, twitter, account, truethevote, reinstate, break, sidney, powell, t...\n",
       "228    [excited, get, work, work, family, family, farmer, team, honor, welcome, el, florista,...\n",
       "78     [theyll, threaten, prosecute, karilake, show, fake, signature, expose, literal, fraud,...\n",
       "Name: text_tokenized, Length: 293, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4af43e",
   "metadata": {},
   "source": [
    "## Dummy Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c16d4",
   "metadata": {},
   "source": [
    "Use Dummy Classifier to predict most frequent label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fb94ef56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:23:48.847683Z",
     "start_time": "2023-02-15T10:23:48.834717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Politics'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X, y)\n",
    "dummy_clf.predict(X)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2d1cc1",
   "metadata": {},
   "source": [
    "Get accuracy of the dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28edeed9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:23:50.737853Z",
     "start_time": "2023-02-15T10:23:50.723788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8310580204778157"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cfa71d",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb0269",
   "metadata": {},
   "source": [
    "Use Tfidfvectorizer to vectorize the tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5d937396",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:23:57.999125Z",
     "start_time": "2023-02-15T10:23:52.596971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#dineshdsouza#</th>\n",
       "      <th>118th</th>\n",
       "      <th>118th congress</th>\n",
       "      <th>able</th>\n",
       "      <th>abortion</th>\n",
       "      <th>access</th>\n",
       "      <th>account</th>\n",
       "      <th>accountable</th>\n",
       "      <th>across</th>\n",
       "      <th>across country</th>\n",
       "      <th>...</th>\n",
       "      <th>write</th>\n",
       "      <th>year</th>\n",
       "      <th>year ago</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.022558</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053728</td>\n",
       "      <td>0.055058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099914</td>\n",
       "      <td>0.111553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017837</td>\n",
       "      <td>0.052457</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130269</td>\n",
       "      <td>0.016811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017346</td>\n",
       "      <td>0.034555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017982</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038184</td>\n",
       "      <td>0.020702</td>\n",
       "      <td>0.082550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081520</td>\n",
       "      <td>0.021757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101220</td>\n",
       "      <td>0.017416</td>\n",
       "      <td>0.024360</td>\n",
       "      <td>0.017971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030446</td>\n",
       "      <td>0.018630</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018482</td>\n",
       "      <td>0.021324</td>\n",
       "      <td>0.008690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015191</td>\n",
       "      <td>0.003718</td>\n",
       "      <td>0.004738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049791</td>\n",
       "      <td>0.051023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055555</td>\n",
       "      <td>0.066457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150903</td>\n",
       "      <td>0.046737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024017</td>\n",
       "      <td>0.068635</td>\n",
       "      <td>0.040851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.087691</td>\n",
       "      <td>0.089863</td>\n",
       "      <td>0.022558</td>\n",
       "      <td>0.024461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079731</td>\n",
       "      <td>0.041157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020986</td>\n",
       "      <td>0.081558</td>\n",
       "      <td>0.015787</td>\n",
       "      <td>0.022082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016887</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019634</td>\n",
       "      <td>0.010645</td>\n",
       "      <td>0.008489</td>\n",
       "      <td>0.175316</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011904</td>\n",
       "      <td>0.028915</td>\n",
       "      <td>0.008955</td>\n",
       "      <td>0.006263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007827</td>\n",
       "      <td>0.004790</td>\n",
       "      <td>0.030516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007066</td>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004637</td>\n",
       "      <td>0.003713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>0.015355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020786</td>\n",
       "      <td>0.009201</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041567</td>\n",
       "      <td>0.019076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>293 rows × 700 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     #dineshdsouza#     118th  118th congress      able  abortion    access  \\\n",
       "0               0.0  0.000000        0.000000  0.000000  0.000000  0.039983   \n",
       "1               0.0  0.053728        0.055058  0.000000  0.099914  0.111553   \n",
       "2               0.0  0.000000        0.000000  0.038184  0.020702  0.082550   \n",
       "3               0.0  0.000000        0.000000  0.001905  0.000000  0.001648   \n",
       "4               0.0  0.049791        0.051023  0.000000  0.055555  0.066457   \n",
       "..              ...       ...             ...       ...       ...       ...   \n",
       "288             0.0  0.087691        0.089863  0.022558  0.024461  0.000000   \n",
       "289             0.0  0.000000        0.000000  0.000000  0.000000  0.000000   \n",
       "290             0.0  0.000000        0.000000  0.019634  0.010645  0.008489   \n",
       "291             0.0  0.000000        0.000000  0.000000  0.000000  0.000000   \n",
       "292             0.0  0.000000        0.000000  0.000000  0.007066  0.005635   \n",
       "\n",
       "      account  accountable    across  across country  ...     write      year  \\\n",
       "0    0.000000     0.000000  0.016451        0.000000  ...  0.000000  0.054472   \n",
       "1    0.000000     0.017837  0.052457        0.021000  ...  0.000000  0.130269   \n",
       "2    0.000000     0.000000  0.081520        0.021757  ...  0.000000  0.101220   \n",
       "3    0.000000     0.000000  0.001356        0.000000  ...  0.018482  0.021324   \n",
       "4    0.000000     0.024795  0.000000        0.000000  ...  0.000000  0.150903   \n",
       "..        ...          ...       ...             ...  ...       ...       ...   \n",
       "288  0.000000     0.021835  0.000000        0.000000  ...  0.000000  0.079731   \n",
       "289  0.024725     0.000000  0.000000        0.000000  ...  0.020986  0.081558   \n",
       "290  0.175316     0.004751  0.000000        0.000000  ...  0.011904  0.028915   \n",
       "291  0.000000     0.000000  0.000000        0.000000  ...  0.000000  0.000000   \n",
       "292  0.000000     0.000000  0.004637        0.003713  ...  0.003951  0.015355   \n",
       "\n",
       "     year ago       yes  yesterday       yet      york       you     young  \\\n",
       "0    0.000000  0.000000   0.000000  0.000000  0.000000  0.018433  0.022558   \n",
       "1    0.016811  0.000000   0.017346  0.034555  0.000000  0.000000  0.017982   \n",
       "2    0.017416  0.024360   0.017971  0.000000  0.000000  0.030446  0.018630   \n",
       "3    0.008690  0.000000   0.001793  0.001786  0.000000  0.015191  0.003718   \n",
       "4    0.046737  0.000000   0.000000  0.024017  0.068635  0.040851  0.000000   \n",
       "..        ...       ...        ...       ...       ...       ...       ...   \n",
       "288  0.041157  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "289  0.015787  0.022082   0.000000  0.000000  0.000000  0.000000  0.016887   \n",
       "290  0.008955  0.006263   0.000000  0.013806  0.000000  0.007827  0.004790   \n",
       "291  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "292  0.000000  0.020786   0.009201  0.003055  0.000000  0.041567  0.019076   \n",
       "\n",
       "        youre  \n",
       "0    0.000000  \n",
       "1    0.000000  \n",
       "2    0.000000  \n",
       "3    0.004738  \n",
       "4    0.000000  \n",
       "..        ...  \n",
       "288  0.000000  \n",
       "289  0.000000  \n",
       "290  0.030516  \n",
       "291  0.067955  \n",
       "292  0.000000  \n",
       "\n",
       "[293 rows x 700 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a vectorizer with max_features=10\n",
    "# (we are using the default token pattern)\n",
    "tfidf = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, \n",
    "                        preprocessor=dummy_fun, token_pattern=None, \n",
    "                        ngram_range=(1,3), min_df=2, max_features=700)\n",
    "\n",
    "# Fit the vectorizer on X_train[\"text\"] and transform it\n",
    "X_train_vectorized = tfidf.fit_transform(X_train)\n",
    "\n",
    "# Visually inspect the vectorized data\n",
    "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e85519",
   "metadata": {},
   "source": [
    "Use a Complement Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "219fb12a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:23:59.520794Z",
     "start_time": "2023-02-15T10:23:59.491680Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natek\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:670: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9281122150789013"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import relevant class and function\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiate a MultinomialNB classifier\n",
    "baseline_model = ComplementNB()\n",
    "\n",
    "# Evaluate the classifier on X_train_vectorized and y_train\n",
    "baseline_cv = cross_val_score(baseline_model, X_train_vectorized, y_train)\n",
    "baseline_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2661ffb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:24:02.287976Z",
     "start_time": "2023-02-15T10:24:01.158675Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the vectorizer on X_train[\"text\"] and transform it\n",
    "X_test_vectorized = tfidf.transform(X_test)\n",
    "\n",
    "# Visually inspect the vectorized data\n",
    "# pd.DataFrame.sparse.from_spmatrix(X_test_vectorized, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3bab5ccf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T10:24:03.778781Z",
     "start_time": "2023-02-15T10:24:03.749788Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natek\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:670: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9591466978375219"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the classifier on X_train_vectorized and y_train\n",
    "baseline_cv = cross_val_score(baseline_model, X_test_vectorized, y_test)\n",
    "baseline_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b92ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1929d15a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f6c626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95ae92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf274718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59020d42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
