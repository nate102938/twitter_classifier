{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports / settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:49:38.432575Z",
     "start_time": "2023-02-22T16:49:38.352187Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\Nate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Nate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import string\n",
    "\n",
    "# Analysis imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP imports\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_colwidth = 90\n",
    "\n",
    "# Downloads (for NLP)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "These are helper functions that assist in the manipulation of tweet strings for pre-processing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:49:40.451027Z",
     "start_time": "2023-02-22T16:49:40.428931Z"
    }
   },
   "outputs": [],
   "source": [
    "def strip_rt_user(text):\n",
    "    if text[0:2] == \"RT\":\n",
    "        colon = text.find(\":\")\n",
    "        return text[colon+1:].lower()\n",
    "    else:\n",
    "        return text.lower()\n",
    "\n",
    "def get_rt_user(text):\n",
    "    if text[0:2] == \"RT\":\n",
    "        colon = text.find(\":\")\n",
    "        user = text[:colon]\n",
    "        at = user.find(\"@\")\n",
    "        return (user[at+1:]).lower()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def addHashTags(text):\n",
    "    return \"#\" + text + \"#\"\n",
    "\n",
    "# Translate nltk POS to wordnet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def remove_characters(text, char_to_remove):\n",
    "    str1 = ''.join(x for x in text if not x in char_to_remove)\n",
    "    return str1\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = remove_characters(text, string.punctuation)\n",
    "    return text\n",
    "\n",
    "def tag_and_lemmatize(text):\n",
    "    newText = text\n",
    "    newText = pos_tag(newText)\n",
    "    newText = [(x[0], get_wordnet_pos(x[1])) for x in newText]\n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "    newText = [(lemma.lemmatize(x[0], x[1])) for x in newText]\n",
    "    return newText\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "# perform all pre-processing on a df\n",
    "def preprocessing(df):\n",
    "    preprocessing_01_model_specific(df)\n",
    "    preprocessing_02_general(df)\n",
    "    preprocessing_03_tag_and_lemmatize(df)\n",
    "    \n",
    "    \n",
    "def preprocessing_01_model_specific(df):\n",
    "    # Copy the RT user name from the text column and put it into a different column.\n",
    "    df['RT_user'] = df['text'].apply(get_rt_user)\n",
    "    df['RT_user'] = df['RT_user'].apply(lambda x: addHashTags(x) if x != \"\" else \"\")\n",
    "\n",
    "    # Pull out the RT user name from the text column\n",
    "    df['text'] = df['text'].apply(strip_rt_user)\n",
    "    \n",
    "def preprocessing_02_general(df):\n",
    "    # Lower case the text tweets\n",
    "    df['text'] = df['text'].str.lower()\n",
    "\n",
    "    # Strip out the meaningless links\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join([n for n in x.split() if n[0:4] != \"http\"]))\n",
    "\n",
    "    # Strip any excess white space\n",
    "    df['text'] = df['text'].apply(lambda x: x.strip())\n",
    "    \n",
    "    # Take out stop words\n",
    "    sw = set(stopwords.words('english'))\n",
    "    sw.update(['amp'])\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join([n for n in x.split() if n not in sw]))\n",
    "\n",
    "    # Remove punctuation\n",
    "    df['text'] = df['text'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "    # Make sure we don't have any random numbers\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join([n for n in x.split() if n.isnumeric() == False]))\n",
    "\n",
    "    # Put together the RT user and the tweet text\n",
    "    df['text'] = df['text'] + \" \" + df['RT_user']\n",
    "\n",
    "    # Make a new column, tokenize the words\n",
    "    df['text_tokenized'] = df['text'].str.split()\n",
    "    \n",
    "    df = df.drop(columns=['id', 'author_id', 'created_at'])\n",
    "    \n",
    "    df['text'] = df['text'].apply(lambda x: np.nan if len(x.strip()) == 0 else x)\n",
    "    df = df.dropna().reset_index(drop=True) \n",
    "\n",
    "    le = LabelEncoder()\n",
    "    df['class_label'] = le.fit_transform(df['class'])\n",
    "    df.head()\n",
    "    \n",
    "def preprocessing_03_tag_and_lemmatize(df):\n",
    "    df['text_tokenized'] = df['text_tokenized'].apply(tag_and_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:33:35.951004Z",
     "start_time": "2023-02-15T05:33:35.933045Z"
    }
   },
   "source": [
    "## Load tweet data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tweet data from file.  Model 1 takes from tweet_list2.csv, which is a scaled down version of all tweet data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:49:44.220081Z",
     "start_time": "2023-02-22T16:49:43.141620Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>class</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.62e+18</td>\n",
       "      <td>On this day 83 years ago, Democrats Delivered the first Social Security checks ever!  ...</td>\n",
       "      <td>2461810448.0</td>\n",
       "      <td>2023-01-31 21:00:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.62e+18</td>\n",
       "      <td>We must keep our children safe from gun violence. Safe storage of guns saves lives and...</td>\n",
       "      <td>2461810448.0</td>\n",
       "      <td>2023-01-30 18:45:49+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.62e+18</td>\n",
       "      <td>Democrats believe that health care is a human right  and #DemocratsDelivered help for ...</td>\n",
       "      <td>2461810448.0</td>\n",
       "      <td>2023-01-28 21:20:12+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.62e+18</td>\n",
       "      <td>Congratulations @PADems for your hard-won victories electing Pennsylvania Democrats wh...</td>\n",
       "      <td>2461810448.0</td>\n",
       "      <td>2023-01-28 04:00:31+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TeamPelosi</td>\n",
       "      <td>Politics - Liberal</td>\n",
       "      <td>1.62e+18</td>\n",
       "      <td>My heart goes out to Tyre Nichols mother and their entire family. Tyre should be alive...</td>\n",
       "      <td>2461810448.0</td>\n",
       "      <td>2023-01-28 02:15:32+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_name               class        id  \\\n",
       "0  TeamPelosi  Politics - Liberal  1.62e+18   \n",
       "1  TeamPelosi  Politics - Liberal  1.62e+18   \n",
       "2  TeamPelosi  Politics - Liberal  1.62e+18   \n",
       "3  TeamPelosi  Politics - Liberal  1.62e+18   \n",
       "4  TeamPelosi  Politics - Liberal  1.62e+18   \n",
       "\n",
       "                                                                                        text  \\\n",
       "0  On this day 83 years ago, Democrats Delivered the first Social Security checks ever!  ...   \n",
       "1  We must keep our children safe from gun violence. Safe storage of guns saves lives and...   \n",
       "2  Democrats believe that health care is a human right  and #DemocratsDelivered help for ...   \n",
       "3  Congratulations @PADems for your hard-won victories electing Pennsylvania Democrats wh...   \n",
       "4  My heart goes out to Tyre Nichols mother and their entire family. Tyre should be alive...   \n",
       "\n",
       "      author_id                 created_at  \n",
       "0  2461810448.0  2023-01-31 21:00:26+00:00  \n",
       "1  2461810448.0  2023-01-30 18:45:49+00:00  \n",
       "2  2461810448.0  2023-01-28 21:20:12+00:00  \n",
       "3  2461810448.0  2023-01-28 04:00:31+00:00  \n",
       "4  2461810448.0  2023-01-28 02:15:32+00:00  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tweets from file\n",
    "tweet_list_file = 'data/tweet_list2.csv'\n",
    "df = pd.read_csv(tweet_list_file)\n",
    "\n",
    "# Format all series as strings\n",
    "for n in df.columns:\n",
    "    df[n] = df[n].astype(str)\n",
    "\n",
    "# Check out the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for nulls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:49:44.741766Z",
     "start_time": "2023-02-22T16:49:44.649142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101074 entries, 0 to 101073\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   user_name   101074 non-null  object\n",
      " 1   class       101074 non-null  object\n",
      " 2   id          101074 non-null  object\n",
      " 3   text        101074 non-null  object\n",
      " 4   author_id   101074 non-null  object\n",
      " 5   created_at  101074 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- There are no null values, which makes sense because I downloaded this data myself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:49:48.805654Z",
     "start_time": "2023-02-22T16:49:48.582250Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- I have some duplicate tweets.  As I noted in the data collection notebook, I must have downloaded some tweets from the same account multiple times while performing the download function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:49:50.295343Z",
     "start_time": "2023-02-22T16:49:49.852035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Duplicates have been deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check class balance at the tweet level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:49:51.745904Z",
     "start_time": "2023-02-22T16:49:51.711956Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Business and finance       21614\n",
       "Science / Technology       15548\n",
       "Politics - Conservative    15500\n",
       "TV / movies                12007\n",
       "Politics - Liberal         12001\n",
       "Sports                     12000\n",
       "Music                      11600\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T06:29:25.411417Z",
     "start_time": "2023-02-15T06:29:25.399488Z"
    }
   },
   "source": [
    "Notes: \n",
    "- It's imbalanced but I'm going to leave it and see if we can still make predictions from the data we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning** This code performs all pre-processing, including lemmatization of the tweet text.  As such, it takes a few minutes to run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-02-22T16:49:56.239Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a copy of the df, leave the original untouched\n",
    "df_pp = df.copy()\n",
    "preprocessing(df_pp)\n",
    "df_model = df_pp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure there's no nulls after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:42:24.938908Z",
     "start_time": "2023-02-22T16:42:24.021Z"
    }
   },
   "outputs": [],
   "source": [
    "df_model.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try to predict the primary interest of the user between our main classifications:\n",
    "- Politics\n",
    "- Sports and entertainment\n",
    "- Business and finance\n",
    "- Science / Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:42:24.942932Z",
     "start_time": "2023-02-22T16:42:24.613Z"
    }
   },
   "outputs": [],
   "source": [
    "df_model.loc[(df_pp['class'] == 'Politics - Conservative') | (df_pp['class'] == 'Politics - Liberal'), 'class'] = 'Politics'\n",
    "df_model.loc[(df_pp['class'] == 'Music') | (df_pp['class'] == 'TV / movies') | (df_pp['class'] == 'Sports'), 'class'] = 'Sports / Entertainment'\n",
    "df_model.loc[(df_pp['class'] == 'Business and finance'), 'class'] = 'Business'\n",
    "df_model = df_model.loc[(df_pp['class'] != 'Travel')]\n",
    "\n",
    "df_model['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T08:01:47.405009Z",
     "start_time": "2023-02-15T08:01:47.395429Z"
    }
   },
   "source": [
    "Aggregate all text words by account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:42:25.717196Z",
     "start_time": "2023-02-22T16:42:25.702324Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ced347adc75b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'text_tokenized'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'sum'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_model' is not defined"
     ]
    }
   ],
   "source": [
    "df_model = df_model.groupby(['class']).agg({'text_tokenized': 'sum'}).reset_index()\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:42:25.957119Z",
     "start_time": "2023-02-22T16:42:25.880Z"
    }
   },
   "outputs": [],
   "source": [
    "df_model.to_csv(\"saved_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:42:26.082833Z",
     "start_time": "2023-02-22T16:42:26.063624Z"
    }
   },
   "outputs": [],
   "source": [
    "# set style for SNS to get higher resolution pics\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':200})\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:42:26.715032Z",
     "start_time": "2023-02-22T16:42:26.694037Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0f57a5574061>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'Business'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_subset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_tokenized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_subset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# sns.set_style(\"darkgrid\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_model' is not defined"
     ]
    }
   ],
   "source": [
    "df_subset = df_model[(df_model['class']=='Business')]\n",
    "df_subset = df_subset.text_tokenized.explode()\n",
    "df_subset.value_counts()\n",
    "# sns.set_style(\"darkgrid\")\n",
    "\n",
    "graphinfo = df_subset.value_counts()[0:12]\n",
    "fix, ax = plt.subplots(figsize=(4.5,3)) \n",
    "ax = sns.barplot(graphinfo.index, graphinfo, ax = ax, color='blue')\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_ylabel(\"count\", fontsize=10)\n",
    "ax.set_xlabel('words', fontsize=10)\n",
    "ax.tick_params(labelsize=10)\n",
    "ax.set_title('Words in Business Tweets', fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sports / Entertainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:42:27.060124Z",
     "start_time": "2023-02-22T16:42:27.028326Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b6e235566457>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'Sports / Entertainment'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_subset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_tokenized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_subset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# sns.set_style(\"darkgrid\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_model' is not defined"
     ]
    }
   ],
   "source": [
    "df_subset = df_model[(df_model['class']=='Sports / Entertainment')]\n",
    "df_subset = df_subset.text_tokenized.explode()\n",
    "df_subset.value_counts()\n",
    "# sns.set_style(\"darkgrid\")\n",
    "\n",
    "\n",
    "graphinfo = df_subset.value_counts()[0:12]\n",
    "fix, ax = plt.subplots(figsize=(4.5,3)) \n",
    "ax = sns.barplot(graphinfo.index, graphinfo, ax = ax, color='blue')\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_ylabel(\"count\", fontsize=10)\n",
    "ax.set_xlabel('words', fontsize=10)\n",
    "ax.tick_params(labelsize=10)\n",
    "ax.set_title('Words in Sports / Entertainment Tweets', fontsize=12);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:42:27.399718Z",
     "start_time": "2023-02-22T16:42:27.381321Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e7f0e745b84c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'Politics'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_subset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_tokenized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_subset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# sns.set_style(\"darkgrid\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_model' is not defined"
     ]
    }
   ],
   "source": [
    "df_subset = df_model[(df_model['class']=='Politics')]\n",
    "df_subset = df_subset.text_tokenized.explode()\n",
    "df_subset.value_counts()\n",
    "# sns.set_style(\"darkgrid\")\n",
    "\n",
    "\n",
    "\n",
    "graphinfo = df_subset.value_counts()[0:12]\n",
    "fix, ax = plt.subplots(figsize=(4.5,3)) \n",
    "ax = sns.barplot(graphinfo.index, graphinfo, ax = ax, color='blue')\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_ylabel(\"count\", fontsize=10)\n",
    "ax.set_xlabel('words', fontsize=10)\n",
    "ax.tick_params(labelsize=10)\n",
    "ax.set_title('Words in Politics Tweets', fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Science / Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:42:27.744197Z",
     "start_time": "2023-02-22T16:42:27.718869Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-86e9ee9ada60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'Science / Technology'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_subset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_tokenized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_subset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# sns.set_style(\"darkgrid\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_model' is not defined"
     ]
    }
   ],
   "source": [
    "df_subset = df_model[(df_model['class']=='Science / Technology')]\n",
    "df_subset = df_subset.text_tokenized.explode()\n",
    "df_subset.value_counts()\n",
    "# sns.set_style(\"darkgrid\")\n",
    "\n",
    "\n",
    "\n",
    "graphinfo = df_subset.value_counts()[0:12]\n",
    "fix, ax = plt.subplots(figsize=(4.5,3)) \n",
    "ax = sns.barplot(graphinfo.index, graphinfo, ax = ax, color='blue')\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_ylabel(\"count\", fontsize=10)\n",
    "ax.set_xlabel('words', fontsize=10)\n",
    "ax.tick_params(labelsize=10)\n",
    "ax.set_title('Words in Science / Technology', fontsize=12);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "329.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
