{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install necessary uncommon packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T22:43:18.901335Z",
     "start_time": "2023-02-22T22:43:14.340007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (3.19.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (1.1.3)\n",
      "Requirement already satisfied: pydub in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (0.3.0)\n",
      "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (2.2.0)\n",
      "Requirement already satisfied: pycryptodome in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (3.17)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (4.5.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (0.23.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (5.3.1)\n",
      "Requirement already satisfied: websockets>=10.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (10.4)\n",
      "Requirement already satisfied: python-multipart in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (0.0.5)\n",
      "Requirement already satisfied: markupsafe in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (1.1.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (8.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (1.18.5)\n",
      "Requirement already satisfied: pydantic in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (1.10.5)\n",
      "Requirement already satisfied: mdit-py-plugins<=0.3.3 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (0.3.3)\n",
      "Requirement already satisfied: aiofiles in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (23.1.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (2023.1.0)\n",
      "Requirement already satisfied: orjson in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (3.8.6)\n",
      "Requirement already satisfied: altair>=4.2.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (4.2.2)\n",
      "Requirement already satisfied: uvicorn in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (0.20.0)\n",
      "Requirement already satisfied: requests in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (2.28.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (3.6.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (3.3.1)\n",
      "Requirement already satisfied: fastapi in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (0.92.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from gradio) (2.11.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from pandas->gradio) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from pandas->gradio) (2.8.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
      "Requirement already satisfied: linkify-it-py<3,>=1; extra == \"linkify\" in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (2.0.0)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from httpx->gradio) (0.16.3)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from httpx->gradio) (1.5.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from httpx->gradio) (2022.12.7)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from python-multipart->gradio) (1.15.0)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from altair>=4.2.0->gradio) (0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from altair>=4.2.0->gradio) (3.2.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from uvicorn->gradio) (0.14.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from uvicorn->gradio) (7.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests->gradio) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests->gradio) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests->gradio) (1.26.14)\n",
      "Requirement already satisfied: yarl<1.6.0,>=1.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from aiohttp->gradio) (1.5.1)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from aiohttp->gradio) (3.0.4)\n",
      "Requirement already satisfied: multidict<5.0,>=4.5 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from aiohttp->gradio) (4.7.5)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from aiohttp->gradio) (3.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from aiohttp->gradio) (20.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->gradio) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->gradio) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->gradio) (2.4.7)\n",
      "Requirement already satisfied: starlette<0.26.0,>=0.25.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from fastapi->gradio) (0.25.0)\n",
      "Requirement already satisfied: uc-micro-py in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from linkify-it-py<3,>=1; extra == \"linkify\"->markdown-it-py[linkify]>=2.0.0->gradio) (1.0.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from httpcore<0.17.0,>=0.15.0->httpx->gradio) (3.6.2)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.17.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (50.3.0.post20201103)\n",
      "Requirement already satisfied: tweepy in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (4.12.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tweepy) (2.28.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nate\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T22:46:01.321440Z",
     "start_time": "2023-02-22T22:46:01.312451Z"
    }
   },
   "outputs": [],
   "source": [
    "deployment_tweet_count = 200\n",
    "\n",
    "model1_max_features = 1250\n",
    "model1_test_size = .4\n",
    "\n",
    "model2_max_features = 1250\n",
    "model2_test_size = .4\n",
    "\n",
    "model1_min_df = 3\n",
    "model2_min_df = 3\n",
    "\n",
    "bearer_key = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports / settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T22:43:22.833371Z",
     "start_time": "2023-02-22T22:43:18.949212Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\Nate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Nate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "# Analysis imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Twitter imports\n",
    "import tweepy\n",
    "\n",
    "# NLP imports\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "# SKlearn imports\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_colwidth = 90\n",
    "\n",
    "# Downloads (for NLP)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger');\n",
    "\n",
    "# Deployment\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are helper functions that assist in the manipulation of tweet strings for pre-processing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T22:46:10.601193Z",
     "start_time": "2023-02-22T22:46:10.584606Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# This is the file we save the twitter bearer key in.  Should be in folder included in gitignore b/c it's private info.\n",
    "twitter_key_file = \"private/twitter_key.json\"\n",
    "\n",
    "# Gets the twitter key from the private folder (not uploaded to github, included in gitignore)\n",
    "def get_twitter_key():\n",
    "    with open(twitter_key_file) as f:\n",
    "        data = json.load(f)\n",
    "    return data['bearer_key']\n",
    "\n",
    "bearer_key = get_twitter_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T22:46:22.036630Z",
     "start_time": "2023-02-22T22:46:22.018856Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_tweets(username, class_, number_of_tweets):\n",
    "    # This is the key to use to download the tweets\n",
    "   \n",
    "    client = tweepy.Client(bearer_token=bearer_key)\n",
    "    user_id = client.get_user(username=username).data.id\n",
    "\n",
    "    # Uses the paginator to request as many tweets as we want (paginator makes it possible to download more than 100 at a time\n",
    "    tweets = []\n",
    "    for tweet in tweepy.Paginator(client.get_users_tweets, user_id, tweet_fields=['created_at', 'author_id'],expansions=[''], max_results=100, exclude=['replies']).flatten(limit=number_of_tweets):\n",
    "        # Scrub the text of any non-readable characters\n",
    "        text = \"\".join(i for i in tweet.text if i in string.printable)\n",
    "        # Scrub the text of any newlines\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        # Put the tweet info into a new dictionary\n",
    "        tweets.append({\n",
    "            \"user_name\"  : str(username),\n",
    "            'class'      : str(class_),\n",
    "            \"id\"         : str(tweet.id),\n",
    "            \"text\"       : str(text),\n",
    "            \"author_id\"  : str(tweet.author_id),\n",
    "            \"created_at\" : str(tweet.created_at)\n",
    "        })\n",
    "    return tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T22:46:34.043033Z",
     "start_time": "2023-02-22T22:46:34.019697Z"
    }
   },
   "outputs": [],
   "source": [
    "def strip_rt_user(text):\n",
    "    if text[0:2] == \"RT\":\n",
    "        colon = text.find(\":\")\n",
    "        return text[colon+1:].lower()\n",
    "    else:\n",
    "        return text.lower()\n",
    "\n",
    "def get_rt_user(text):\n",
    "    if text[0:2] == \"RT\":\n",
    "        colon = text.find(\":\")\n",
    "        user = text[:colon]\n",
    "        at = user.find(\"@\")\n",
    "        return (user[at+1:]).lower()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def addHashTags(text):\n",
    "    return \"#\" + text + \"#\"\n",
    "\n",
    "# Translate nltk POS to wordnet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def remove_characters(text, char_to_remove):\n",
    "    str1 = ''.join(x for x in text if not x in char_to_remove)\n",
    "    return str1\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = remove_characters(text, string.punctuation)\n",
    "    return text\n",
    "\n",
    "def tag_and_lemmatize(text):\n",
    "    newText = text\n",
    "    newText = pos_tag(newText)\n",
    "    newText = [(x[0], get_wordnet_pos(x[1])) for x in newText]\n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "    newText = [(lemma.lemmatize(x[0], x[1])) for x in newText]\n",
    "    return newText\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "# perform all pre-processing on a df\n",
    "def preprocessing(df):\n",
    "    preprocessing_01_model_specific(df)\n",
    "    preprocessing_02_general(df)\n",
    "    preprocessing_03_tag_and_lemmatize(df)\n",
    "    \n",
    "    \n",
    "def preprocessing_01_model_specific(df):\n",
    "    # Copy the RT user name from the text column and put it into a different column.\n",
    "    df['RT_user'] = df['text'].apply(get_rt_user)\n",
    "    df['RT_user'] = df['RT_user'].apply(lambda x: addHashTags(x) if x != \"\" else \"\")\n",
    "\n",
    "    # Pull out the RT user name from the text column\n",
    "    df['text'] = df['text'].apply(strip_rt_user)\n",
    "    \n",
    "def preprocessing_02_general(df):\n",
    "    # Lower case the text tweets\n",
    "    df['text'] = df['text'].str.lower()\n",
    "\n",
    "    # Strip out the meaningless links\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join([n for n in x.split() if n[0:4] != \"http\"]))\n",
    "\n",
    "    # Strip any excess white space\n",
    "    df['text'] = df['text'].apply(lambda x: x.strip())\n",
    "    \n",
    "    # Take out stop words\n",
    "    sw = set(stopwords.words('english'))\n",
    "    sw.update(['amp'])\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join([n for n in x.split() if n not in sw]))\n",
    "\n",
    "    # Remove punctuation\n",
    "    df['text'] = df['text'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "    # Make sure we don't have any random numbers\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join([n for n in x.split() if n.isnumeric() == False]))\n",
    "\n",
    "    # Put together the RT user and the tweet text\n",
    "    df['text'] = df['text'] + \" \" + df['RT_user']\n",
    "\n",
    "    # Make a new column, tokenize the words\n",
    "    df['text_tokenized'] = df['text'].str.split()\n",
    "    \n",
    "    df = df.drop(columns=['id', 'author_id', 'created_at'])\n",
    "    \n",
    "    df['text'] = df['text'].apply(lambda x: np.nan if len(x.strip()) == 0 else x)\n",
    "    df = df.dropna().reset_index(drop=True) \n",
    "\n",
    "    le = LabelEncoder()\n",
    "    df['class_label'] = le.fit_transform(df['class'])\n",
    "    df.head()\n",
    "    \n",
    "def preprocessing_03_tag_and_lemmatize(df):\n",
    "    df['text_tokenized'] = df['text_tokenized'].apply(tag_and_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T22:46:34.262908Z",
     "start_time": "2023-02-22T22:46:34.253044Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_primary_interest(username):\n",
    "\n",
    "    try:\n",
    "        print(\"Attempting to get \", deployment_tweet_count, \" tweets from handle '\", username, \"'\", sep=\"\")\n",
    "        tweets = get_tweets(username, 'blank', deployment_tweet_count)\n",
    "    except:\n",
    "        print(\"No tweets were returned due to an API error.\")\n",
    "    else:\n",
    "        if len(tweets) > 0:\n",
    "            df_new = pd.DataFrame.from_dict(tweets)\n",
    "            preprocessing(df_new)\n",
    "            df_new = df_new.groupby(['user_name', 'class']).agg({'text_tokenized': 'sum'}).reset_index()\n",
    "\n",
    "            #load the content\n",
    "            tfidf = pickle.load(open(\"models/model1_tfidf.pkl\", \"rb\" ))\n",
    "            model = pickle.load(open(\"models/model1_model.pkl\", \"rb\"))\n",
    "\n",
    "            tf_new = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, \n",
    "                                    preprocessor=dummy_fun, token_pattern=None, \n",
    "                                    ngram_range=(1,3), min_df=model1_min_df, max_features=model1_max_features, vocabulary=tfidf.vocabulary_)\n",
    "            df_new_vectorized = tf_new.fit_transform(df_new['text_tokenized'])\n",
    "\n",
    "            category1 = model.predict(df_new_vectorized)[0]\n",
    "            if category1 != \"Politics\":\n",
    "                return (category1)\n",
    "            else:\n",
    "                #load the content\n",
    "                tfidf = pickle.load(open(\"models/model2_tfidf.pkl\", \"rb\" ))\n",
    "                model = pickle.load(open(\"models/model2_model.pkl\", \"rb\"))\n",
    "\n",
    "                tf_new = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, \n",
    "                                    preprocessor=dummy_fun, token_pattern=None, \n",
    "                                    ngram_range=(1,3), min_df=model2_min_df, max_features=model2_max_features, vocabulary=tfidf.vocabulary_)\n",
    "                df_new_vectorized = tf_new.fit_transform(df_new['text_tokenized'])\n",
    "\n",
    "                category2 = model.predict(df_new_vectorized)[0]\n",
    "                return (category2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T22:46:37.926665Z",
     "start_time": "2023-02-22T22:46:35.646414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to get 200 tweets from handle 'barackobama'\n",
      "Primary interest: Politics - Liberal\n"
     ]
    }
   ],
   "source": [
    "handle = \"barackobama\"\n",
    "print(\"Primary interest:\", get_primary_interest(handle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create web app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T18:24:52.336822Z",
     "start_time": "2023-02-22T18:24:47.367494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://8c089b51656e1ab36a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://8c089b51656e1ab36a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_interest_(name):\n",
    "    return get_primary_interest(name)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    handle = gr.Textbox(label=\"Twitter handle\")\n",
    "    output = gr.Textbox(label=\"Primary interest\")\n",
    "    get_interest = gr.Button(\"Get user's primary interest\")\n",
    "    get_interest.click(fn=get_interest_, inputs=handle, outputs=output)\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "480px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
